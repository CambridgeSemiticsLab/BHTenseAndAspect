{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Time Constructions\n",
    "\n",
    "In this notebook we construct a rudimentary parser for parsing the various roles of Hebrew time phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.12\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "119 features found and 6 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n",
      "  5.03s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src:\n",
       "    local(\"SILEOT.ttf\"),\n",
       "    url(\"https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true\");\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.lno,.catom.lno,.patom.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.rno,.catom.rno,.patom.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 2;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "\n",
       "tr.tf, td.tf, th.tf {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import copy\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance as lev_dist\n",
    "from pprint import pprint\n",
    "\n",
    "# local packages\n",
    "from tf_tools.load import load_tf\n",
    "\n",
    "# load semantic vectors\n",
    "from locations import semvector\n",
    "with open(semvector, 'rb') as infile: \n",
    "    semdist = pickle.load(infile)\n",
    "\n",
    "# load and configure Text-Fabric\n",
    "TF, api, A = load_tf()\n",
    "F, E, T, L = api.F, api.E, api.T, api.L\n",
    "A.displaySetup(condenseType='phrase', withNodes=True, extraFeatures='st')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery\n",
    "\n",
    "We could use some machinery to do the hard work of looking in and around a node. In the older approach we used TF search templates. But these are not very efficient at scale, and they are always bound by the limits of the query language. I take another approach here: a set of classes that specify locations and directions within a specified context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from positions import Positions, PositionsTF, Walker, Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Positions(TF)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Positions` class enables concise access to adjacent nodes within a given context. This allows us to write algorithms with query-like efficiency with all of the power of Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is instantiated on a word node and can provide contextual look-up data for a given word. For example, given a phrase containing the following word nodes:\n",
    "\n",
    "> (189681, 189682, **189683**, 189684, 189685, 189686) <br>\n",
    "\n",
    "representing the following phrase (space separated for clarity):\n",
    "\n",
    "> ב שׁנת **שׁלשׁים** ו שׁמנה שׁנה\n",
    "\n",
    "Given that the bolded node, `189683` is our `source` word, we instantiate the class, feeding in the node, the \"phrase_atom\" string (which is the context we want to search within), and an instance of Text-Fabric (`tf`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "      #    source node    context  TF instance  \n",
    "      #         |            |       |\n",
    "P = PositionsTF(189683, 'phrase_atom', A).get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to obtain the word adjacent one space forward, we simply ask `P` for `1`, which gives us the next word in the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189684"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to ask for 4 words forward, we go beyond the bounds of the phrase. But `P` handles this by returning nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look back one word, we simply give a negative value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189682"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `P` can be used to quickly call features on these words. For instance, in order to get the lexeme of the word two words in front of `189683`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CMNH/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2,'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to get a number of features, we can just add other features to the arguments. The result is a feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CMNH/', 'sg'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2, 'lex', 'nu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P` can also handle features on the source node itself by giving a positionality of `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLC/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(0, 'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Positions` also exists in a non-TF version\n",
    "\n",
    "When the non-tf version of `Positions` is provided any iterable, it can perform the same functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ps = ['The', 'good', 'dog', 'jumped.']\n",
    "\n",
    "P = Positions('good', test_ps).get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positions can perform a function on the result with an option `do`. In the example below, the word two words ahead is found and an upper-case function is called on the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JUMPED.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2, do=lambda w: w.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-tf version of `Positions` makes it possible to do positionality searches with any ordered list of Python objects that represent linguistic units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Walker`\n",
    "\n",
    "`Walker` performs a similar function to `Positions`, except it is ambiguous to exact positions, walking either `ahead` or `back` from the source to a target node in the context. A function must be supplied that returns `True` on the target node.\n",
    "\n",
    "We instantiate the `Walker` using the same source and context as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 189683\n",
    "# get words inside source's phrase_atom\n",
    "positions = L.d(\n",
    "    L.u(189683,'phrase_atom')[0], 'word'\n",
    ")\n",
    "\n",
    "Wk = Walker(source, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Walker` is demonstrated below with the same word. A simple `lambda` function is used to test for the lexeme. In the example below, we find the first word ahead of `189683` that is a cardinal number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189685"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative demonstrates the `None` returned on the lack of a valid match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'BOOGABOOGA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example wherein we walk backwards to the preposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189681"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.back(lambda w: F.sp.v(w) == 'prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify that the walk should be interrupted under certain conditions with a `stop` function. In this case we walk forward to the next cardinal number, but the walk is interrupted when the `stop` function detects a conjunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card',\n",
    "         stop=lambda w: F.sp.v(w) == 'conj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the opposite with a `go` function argument, which defines the nodes that allowed to intervene between `source` and `target`. Below we specify that *only* a conjunction should intervene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189685"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card',\n",
    "         go=lambda w: F.sp.v(w) == 'conj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `go` and `stop` functions can be as permissive or strict as desired.\n",
    "\n",
    "Finally, we can tell `Walker` that the output of the validation function should be returned instead of the node itself with the optional argument `output=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'card'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_funct = lambda w: F.ls.v(w) if F.ls.v(w)=='card' else None\n",
    "\n",
    "Wk.ahead(val_funct, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ability is useful for certain tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `Positions`, `Walker` can be used in non-TF contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ps = ['The', 'bad', 'cat', 'swatted.']\n",
    "\n",
    "Wk_notf = Walker('bad', test_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swatted.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk_notf.ahead(lambda w: w.startswith('sw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning All Results along Path\n",
    "\n",
    "`Walker` can also return all results along the path by toggling `every=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'swatted.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk_notf.ahead(lambda w: type(w)==str, every=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dummy`\n",
    "\n",
    "When writing conditions and logic, we want an object that passively receives `NoneType`s or zero `int`s without throwing errors. Such an object should also return `None` to reflect its `False` value. `Dummy`, provides such functionality. `Dummy` can receive all of the arguments, kwargs, and function calls as a `Positions` or `Walker` object. But it returns absolutely nothing. Ouch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Dummy(None, 'phrase_atom', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function call below returns `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.get(1, 'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.ahead(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D` is essentially a souless void that consumes whatever you throw at it and gives nothing in return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For safe-calls on a `Position` or `Walker` object, assign nodes to it via a function with a `Dummy` given on null nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPos(node, context, tf):\n",
    "    \"\"\"A function to get Positions safely.\"\"\"\n",
    "    if node:\n",
    "        return PositionsTF(node, context, tf)\n",
    "    else:\n",
    "        return Dummy() # <- give dummy on empty node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getPos(None, 'phrase_atom', A)\n",
    "P.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = getPos(1, 'phrase_atom', A)\n",
    "P.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need for Semantic Data\n",
    "\n",
    "The accurate processing of word connections depends on fuller semantic data than BHSA provides. Future semantic data could be stored in a similar way to word sets (`wsets`). \n",
    "\n",
    "For example, in the two phrases\n",
    "\n",
    "> (Exod 25:39) ככר זהב טהור <br>\n",
    "> (2 Sam 24:24) בכסף שקלים חמשׁים\n",
    "\n",
    "we see that זהב and כסף, despite being in two different positions with two different words indicates a kind of \"composed of\" semantic concept: \"round gold\" (i.e. round composed of gold) and \"silver shekels\" (shekels composed of silver). To process these kinds of links, we need a list of nouns that often function as \"material.\" But this is only the beginning. Many other words will have specific semantic values that motivate their syntactic behavior. Such a scope lies outside the bounds of this author's current project on Hebrew time phrases.\n",
    "\n",
    "## A Compromise: Time Phrases\n",
    "\n",
    "Since constructing these semantic classes is vastly time consuming, I want to start with a smaller set of cases. I will instead focus on parsing connections within time phrases for now. This is because I am analyzing time phrases in my current ongoing PhD project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint(ph):\n",
    "    \"\"\"Isolate phrases with gaps.\"\"\"\n",
    "    ph = L.d(ph,'word')\n",
    "    for w in ph:\n",
    "        if ph[-1] == w:\n",
    "            break\n",
    "        elif (ph[ph.index(w)+1] - w) > 1:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864 phrases ready\n"
     ]
    }
   ],
   "source": [
    "alltimes = [\n",
    "    ph for ph in F.otype.s('timephrase') \n",
    "]\n",
    "    \n",
    "timephrases = [ph for ph in alltimes if not disjoint(ph)]\n",
    "\n",
    "print(f'{len(timephrases)} phrases ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search & Display Functions\n",
    "\n",
    "The functions below allow for fast searching and displaying of queries using a `Construction` object, described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cx_analysis.search import SearchCX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cx_show = SearchCX(A)\n",
    "pretty, prettyconds, showcx, search = (\n",
    "    cx_show.pretty, cx_show.prettyconds, \n",
    "    cx_show.showcx, cx_show.search\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Classes\n",
    "\n",
    "* `Construction` - an object that represents a linguistic construction; the class records roles and the words that occupy them, as well as has methods for accessing and retrieving data on embedded roles/other constructions\n",
    "* `CXBuilder` - matches conditions to build `Construction` objects; populates them with requisite data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cx_analysis.cx import Construction\n",
    "from cx_analysis.build import CXbuilder, CXbuilderTF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Constructions\n",
    "\n",
    "The `wordConstructions` builder class recognizes word semantic classes and types based on provided criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cx_analysis.word_grammar import Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subphrase Constructions\n",
    "\n",
    "The `SPConstructions` class prepares subphrase constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cx_analysis.phrase_grammar import Subphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning word construction analysis...\n",
      "\t0:00:07.750267 COMPLETE \t[ 12887 ] words loaded\n"
     ]
    }
   ],
   "source": [
    "words = Words(A) # word CX builder\n",
    "\n",
    "# analyze all matches; return as dict\n",
    "start = datetime.now()\n",
    "print(f'Beginning word construction analysis...')\n",
    "wordcxs = words.cxdict(\n",
    "    s for tp in timephrases\n",
    "        for s in L.d(tp,'word')\n",
    ")\n",
    "print(f'\\t{datetime.now() - start} COMPLETE \\t[ {len(wordcxs)} ] words loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time phrase CX builder\n",
    "spc = Subphrases(wordcxs, semdist, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pickle Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tuplify_graph(node):\n",
    "#     \"\"\"Convert a NetworkX constructional graph into a tuple\"\"\"\n",
    "#     return tuple(\n",
    "#         (n1, n2, {'role': node.graph[n1][n2]['role']})\n",
    "#              for n1, n2 in nx.bfs_edges(node.graph, node)\n",
    "#     )\n",
    "\n",
    "# def graphify_tuple(graph_tuple):\n",
    "#     \"\"\"Convert a graph tuple back into NetworkX graph\"\"\"\n",
    "#     return nx.Digraph(graph_tuple)\n",
    "\n",
    "# def package_graph(node, graph=None):\n",
    "#     \"\"\"Recursively turn all cxs and contained cxs into tuples\"\"\"\n",
    "    \n",
    "#     # map graph nodes to tuple\n",
    "#     if graph is None:\n",
    "#         new_graph = tuplify_graph(node)\n",
    "#         # bequeath to all embedded nodes\n",
    "#         for nn in node.graph:        \n",
    "#             if type(nn) == Construction and type(nn.graph) != tuple:\n",
    "#                 package_graph(nn, graph=new_graph)\n",
    "#         node.graph = new_graph\n",
    "    \n",
    "#     # assign to tuple\n",
    "#     else:\n",
    "#         node.graph = graph\n",
    "        \n",
    "# def unpackage_graph(node, graph=None):\n",
    "#     \"\"\"Recursively turn all tupled graphs into NetworkX graphs\"\"\"\n",
    "    \n",
    "#     # map graph nodes to tuple\n",
    "#     if graph is None:\n",
    "#         node.graph = nx.DiGraph(node.graph)\n",
    "#         # bequeath to all embedded nodes\n",
    "#         for nn in node.graph:        \n",
    "#             if type(nn) == Construction and type(nn.graph) == tuple:\n",
    "#                 unpackage_graph(nn, graph=node.graph)\n",
    "    \n",
    "#     # assign to tuple\n",
    "#     else:\n",
    "#         node.graph = graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TO-FIX\n",
    "\n",
    "* missed appo 361457 cx: 1450112 (בחים מספר ימי חיי הבלו)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pretty(1448320)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_small = spc.appo_name(202679)\n",
    "#showcx(test_small, conds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On deck: adjectival preposition\n",
    "# check performance: 1448556\n",
    "\n",
    "test = spc.analyzestretch(L.d(1450075, 'word'), debug=False)\n",
    "\n",
    "# for res in test:\n",
    "#     showcx(res, conds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [w for ph in timephrases for w in L.d(ph, 'word')]\n",
    "\n",
    "# results = search(words, spc.appo_name, pattern='entity_name', show=100, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for res in results:\n",
    "#     head, appo = list(res.getsuccroles('head'))[-1], list(res.getsuccroles('appo'))[-1]\n",
    "#     hlex, alex = F.lex.v(int(head)), F.lex.v(int(appo))\n",
    "    \n",
    "#     showcx(res)\n",
    "#     print()\n",
    "#     print(f'lexs: {hlex} x {alex}')\n",
    "#     print(f'dist: {semdist[hlex][alex]}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Tests on Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements = sorted(set(L.u(res.element, 'timephrase')[0] for res in results))\n",
    "\n",
    "# for el in elements:\n",
    "    \n",
    "#     stretch = L.d(el, 'word')\n",
    "#     test = spc.analyzestretch(stretch)\n",
    "    \n",
    "#     for res in test:\n",
    "#         showcx(res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Random Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuff = [k for k in timephrases\n",
    "#             if len(L.d(k,'word')) > 4]\n",
    "# random.shuffle(shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phrase in shuff[:25]:\n",
    "    \n",
    "#     print('analyzing', phrase)\n",
    "#     elements = L.d(phrase,'word')\n",
    "    \n",
    "#     try:\n",
    "#         cxs = tpc.analyzestretch(elements)\n",
    "#         if cxs:\n",
    "#             for cx in cxs:\n",
    "#                 showcx(cx, refslots=elements)\n",
    "#         else:\n",
    "#             showcx(Construction(), refslots=elements)\n",
    "    \n",
    "#     except:\n",
    "#         sys.stderr.write(f'\\nFAIL...running with debug...\\n')\n",
    "#         pretty(phrase)\n",
    "#         tpc.analyzestretch(elements, debug=True)\n",
    "#         raise Exception('...debug complete...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on All Timephrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000046 beginning analysis...\n",
      "\t0:00:15.457987\tdone with iter 500/3864\n",
      "\t0:00:32.351404\tdone with iter 1000/3864\n",
      "\t0:00:50.386257\tdone with iter 1500/3864\n",
      "\t0:01:05.639771\tdone with iter 2000/3864\n",
      "\t0:01:24.600140\tdone with iter 2500/3864\n",
      "\t0:01:44.746979\tdone with iter 3000/3864\n",
      "\t0:01:59.328218\tdone with iter 3500/3864\n",
      "0:02:13.405888\tCOMPLETE\n",
      "--------------------\n",
      "3864 phrases matched with Constructions...\n",
      "0 phrases not yet matched with Constructions...\n"
     ]
    }
   ],
   "source": [
    "phrase2cxs = collections.defaultdict(list)\n",
    "nocxs = []\n",
    "\n",
    "# time it\n",
    "start = datetime.now()\n",
    "\n",
    "print(f'{datetime.now()-start} beginning subphrase analysis...')\n",
    "\n",
    "for i, phrase in enumerate(timephrases):\n",
    "     \n",
    "    # analyze all known relas\n",
    "    elements = L.d(phrase,'word')\n",
    "    \n",
    "    # analyze with debug exceptions\n",
    "    try:\n",
    "        cxs = spc.analyzestretch(elements)\n",
    "    except:\n",
    "        sys.stderr.write(f'\\nFAIL...running with debug...\\n')\n",
    "        pretty(phrase)\n",
    "        spc.analyzestretch(elements, debug=True)\n",
    "        raise Exception('...debug complete...')\n",
    "\n",
    "    # save those phrases that have no matching constructions\n",
    "    if not cxs:\n",
    "        nocxs.append(phrase)\n",
    "    else:\n",
    "        phrase2cxs[phrase] = cxs\n",
    "        \n",
    "    # report status\n",
    "    if i % 500 == 0 and i:\n",
    "        print(f'\\t{datetime.now()-start}\\tdone with iter {i}/{len(timephrases)}')\n",
    "        \n",
    "print(f'{datetime.now()-start}\\tCOMPLETE')\n",
    "print('-'*20)\n",
    "print(f'{len(phrase2cxs)} phrases matched with Constructions...')\n",
    "print(f'{len(nocxs)} phrases not yet matched with Constructions...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Gaps\n",
    "\n",
    "### Identify Gaps\n",
    "\n",
    "Find timephrases that contain un-covered words besides waw conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gapped = []\n",
    "# tested = []\n",
    "\n",
    "# for ph, cxs in phrase2cxs.items():\n",
    "    \n",
    "#     tested.append(ph)\n",
    "    \n",
    "#     ph_slots = set(\n",
    "#         s for s in L.d(ph,'word')\n",
    "#     )\n",
    "#     cx_slots = set(\n",
    "#         s for cx in cxs\n",
    "#             for s in cx.slots\n",
    "#     )\n",
    "    \n",
    "#     if ph_slots.difference(cx_slots):\n",
    "#         gapped.append(cxs)\n",
    "        \n",
    "# print(f'{len(gapped)} gapped phrases logged...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for gp in gapped[:25]:\n",
    "#     for cx in gp:\n",
    "#         showcx(cx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Constructions\n",
    "\n",
    "Developing a CXbuilder to connect all constructions in a complete phrase.\n",
    "\n",
    "\n",
    "### Ambiguity with Coordinate CXs\n",
    "\n",
    "Considerable ambiguity is present in several coordinate constructions:\n",
    "\n",
    "**`A B and C`**<br>\n",
    "Given A, B, C == nominal words. Is their relationship `A // B // C` or `A+B // C`. In other words: **what is the relationship of two adjacent nominal words given a list?** Is B a descriptor of A or is it an independent element? \n",
    "\n",
    "**`A of B and C`**<br>\n",
    "Is it, `(A of B) // (C)` or `(A of (B // C)`\n",
    "\n",
    "Or even:\n",
    "\n",
    "**`A of B C and D`**<br>\n",
    "This pattern combines elements from both ambiguous cases.\n",
    "\n",
    "### Method\n",
    "\n",
    "To address these ambiguities we will apply a battery of disambiguation attempts. At the core of these attempts is a [Semantic Vector Space](https://en.wikipedia.org/wiki/Vector_space_model), which is able to quantify the semantic distance between two words based on their contextual uses throughout the Hebrew Bible.\n",
    "\n",
    "The working hypothesis of this method is\n",
    "> Words in coordination with each other will be more semantically similar (i.e. the least distance in the vector space) than other candidates in the phrase.\n",
    "\n",
    "Semantic similarity in a vector space is not the only method used, however. Another aspect of semantic closeness is phrase structure. For instance, the identity of phrase types is taken into consideration above semantic similarity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'phrase2cxs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-55-be9e66ba0cec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcx_analysis\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mphrase_grammar\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPhrases\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphrase2cxs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msemdist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mA\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'phrase2cxs' is not defined"
     ]
    }
   ],
   "source": [
    "from cx_analysis.phrase_grammar import Phrases\n",
    "cxp = Phrases(phrase2cxs, semdist, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A.show(A.search('''\n",
    "\n",
    "# timephrase\n",
    "#     word pdp=subs ls#card|prpe lex#KL/|JWM/ st=a\n",
    "\n",
    "#     <: word lex=JWM/\n",
    "# ''')[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following phrases contain cases that still\n",
    "# need to be fixed for the coordinate cx; some should\n",
    "# actually be done in the previous cx builder at subphrase level\n",
    "\n",
    "to_fix = [\n",
    "    1450039, # coord, add adjacent advb cx with JWM\n",
    "    1450647, # coord, consider prioritizing Levenshtein over size\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CX prep_ph (284192, 284193, 284194, 284195, 284196), CX cont (284197,)]"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testph = phrase2cxs[1449445]\n",
    "testph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cxp.appo(testph[1])\n",
    "\n",
    "# showcx(test, conds=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "testph = phrase2cxs[1446841]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cxp.analyzestretch(\n",
    "#     testph, \n",
    "#     duplicate=True,\n",
    "#     debug=True)\n",
    "\n",
    "# for res in test:\n",
    "#     showcx(res, conds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>\n",
    "\n",
    "# TOFIX:\n",
    "* fix apposition - 1447545 (צען מצרים)\n",
    "\n",
    "# TOTEST: \n",
    "\n",
    "1450333 - from apposition to proper name\n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_gaps(cx):\n",
    "    \"\"\"Isolate cxs with gaps\"\"\"\n",
    "    timephrase = L.u(next(iter(cx.slots)),'phrase')[0]\n",
    "    if set(L.d(timephrase,'word')) - cx.slots:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def filt(cx):\n",
    "    \"\"\"Find specific lexeme\"\"\"\n",
    "    timephrase = L.u(next(iter(cx.slots)),'phrase')[0]\n",
    "    phrasewords = L.d(timephrase, 'word')\n",
    "    if (\n",
    "        {'JWM/', 'LJLH/'}.issubset(set(F.lex.v(w) for w in phrasewords))\n",
    "        and len(phrasewords) == 3\n",
    "    ):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements = [\n",
    "#     cx for ph in list(phrase2cxs.values())\n",
    "#         for cx in ph\n",
    "# ]\n",
    "\n",
    "# results = search(\n",
    "#     elements, \n",
    "#     cxp.appo, \n",
    "#     pattern='PP',\n",
    "#     shuffle=False,\n",
    "#     #select=lambda c: filt(c),\n",
    "#     extraFeatures='lex st',\n",
    "#     show=150\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Tests\n",
    "\n",
    "Testing across a whole phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cxp.analyzestretch(phrase2cxs[1449168], debug=True)\n",
    "# for res in test:\n",
    "#     showcx(res, conds=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

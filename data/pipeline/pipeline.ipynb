{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "\n",
    "This module describes in detail the data-production pipeline of the Time Collocations project. The root source of the data is the [BHSA](https://www.github.com/etcbc/bhsa) of the Eep Talstra Centre for Bible and Computer, Vrije Universiteit Amsterdam. The BHSA contains grammatical and syntactic annotations, including a feature called `Function`, which identifies adverbial time phrases.\n",
    "\n",
    "This project makes four primary kinds of modifications to the root BHSA data:\n",
    "\n",
    "1. **Some phrases marked for time adverbial function are corrected and remapped to the appropriate function.** These corrections are based on examining the data manually and deciding whether the default BHSA label should be kept or modified. Those cases will be described explicitly below.\n",
    "2. **Meaningful groups above or below a phrase or other formal object are \"chunked\" into new objects, called `chunk`.** The `chunk` object contains word groups such as quantifier number chains (below the phrase level) or time adverbial chunks (units above the phrase level). This latter case consists of certain situations where BHSA splits a time adverbial phrase into two separate phrases. Those two parts are recombined in `chunks` for further analysis.\n",
    "3. **Construction objects are generated**. This is the final step in the pipeline, which is based on the statistical and functional analysis of time adverbials. A series of new objects will be built using the `chunk` objects. They will contain labels for semantic roles and semantic function. These labels are to be constructed using inductive, data-driven analysis combined with insights from Construction Grammar.\n",
    "4. **New features on phrases, chunks, or constructions.** These are various features stored on the objects noted above.\n",
    "\n",
    "**The end result is a custom database, which consists of original BHSA data, modified BHSA phrase function data, and new objects with their associated features**.\n",
    "\n",
    "The whole pipeline is represented below in diagram form. \n",
    "\n",
    "<table>\n",
    "<img src=\"../../docs/images/pipeline_diagram.png\" width=\"50%\" height=\"50%\" align=\"middle\">\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import needed packages and modules\n",
    "import os, sys, collections, glob\n",
    "from tf.fabric import Fabric\n",
    "from tf.app import use\n",
    "show_examples = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For visualizing the data, I load an instance of text-fabric with BHSA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_examples:\n",
    "    bhsa = use('bhsa', silent=True)\n",
    "    F, T, L = bhsa.api.F, bhsa.api.T, bhsa.api.L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Import Pipeline Classes\n",
    "\n",
    "The classes below enact the pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from remapfunctions import RemapFunctions # remaps phrase functions\n",
    "from chunking import Chunker # chunks meaningful groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input and output files are outlined below. Both directories consist (or will) of .tf resources. The input files are the base BHSA dataset while the output will be the customized, project dataset. Another dataset, `heads`, which is crucial for this work, is identified below as well. See the documentation for heads [here](https://nbviewer.jupyter.org/github/ETCBC/heads/blob/master/phrase_heads.ipynb)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "home = os.path.expanduser('~/')\n",
    "bhsa_dir = os.path.join(home, 'text-fabric-data/etcbc/bhsa/tf/c') # input\n",
    "heads_dir = os.path.join(home, 'github/etcbc/heads/tf/c') # heads data\n",
    "output_dir = '../tf' # output; all new data goes here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Purge Old .tf Files\n",
    "\n",
    "Remove all .tf files in the output directory. This is necessary since all output data goes to the same place, and subsequent classes depend on previous data. Without the purge, subsequent runs will add new objects on top of previous ones!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Purging old data...\n",
      "\tpurging ../tf/function.tf\n",
      "\tpurging ../tf/note.tf\n",
      "\tpurging ../tf/label.tf\n",
      "\tpurging ../tf/otype.tf\n",
      "\tpurging ../tf/role.tf\n",
      "\tpurging ../tf/oslots.tf\n",
      "\tDONE\n"
     ]
    }
   ],
   "source": [
    "print('Purging old data...')\n",
    "old_tf = glob.glob(os.path.join(output_dir, '*.tf'))\n",
    "for file in old_tf:\n",
    "    print(f'\\tpurging {file}')\n",
    "    os.remove(file)\n",
    "print('\\tDONE')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The metadata below is assigned to all new features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_metadata = {'source': 'https://github.com/etcbc/bhsa',\n",
    "                 'origin': 'Made by the ETCBC of the Vrije Universiteit Amsterdam; edited by Cody Kingham'}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Phrase Function Edits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "remap_functs = RemapFunctions(bhsa_dir=bhsa_dir, \n",
    "                              export_dir=output_dir, \n",
    "                              metadata=base_metadata)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following functions will be changed..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if show_examples:\n",
    "    for phrase, newfunct in remap_functs.newfunctions.items():\n",
    "        print(f'{phrase} node with function {F.function.v(phrase)} will be remapped to {newfunct}')\n",
    "        bhsa.pretty(phrase)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change is executed below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updating phrase functions...\n",
      "\tadding notes feature to new functions...\n",
      "\texporting tf...\n",
      "   |     0.36s T function             to /Users/cody/github/csl/time_collocations/data/tf\n",
      "   |     0.00s T note                 to /Users/cody/github/csl/time_collocations/data/tf\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "remap_functs.execute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Generate Chunk Objects\n",
    "\n",
    "The ETCBC data is not granular enough for many types of searches beneath the phrase level. For example, if there are coordinated noun phrases that all function as a single phrase, the individual phrases are not delineated. I need them spliced out so that I can track coordinated nouns in a phrase. Another case is with quantifiers, wherein the quantifier chains themselves are not in any way set apart from other items in the phrase. For these cases, I will make `chunk` objectsâ€”these are essentially phrase-like objects. Another problem is that some phrases are split into two, whereas elsewhere in the database the same phrase pattern is portrayed as a single phrase. This is fixed by creating a new `chunk` object.\n",
    "\n",
    "`chunk` objects have two important features, called `label` and `role`. The `label` feature is essentially the name of the function. For instance, `timephrase` or `quant` (quantifier). The feature `role` is an edge feature, which maps the chunk's component parts to the new object. For example, in a `quant_NP`, there is an edge drawn from the noun to the `quant_NP` chunk; the edge has a value of \"quantified\" since the noun is the quantified item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunker = Chunker(bhsa_dir=bhsa_dir, \n",
    "                  heads_dir=heads_dir,\n",
    "                  output_dir=output_dir, \n",
    "                  metadata=base_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running chunkers...\n",
      "\trunning time chunker...\n",
      "\trunning quant chunker...\n",
      "\trunning prep chunker...\n",
      "82289 chunk objects formed...\n",
      "\t73963 chunk objects with label prep\n",
      "\t4072 chunk objects with label timephrase\n",
      "\t3196 chunk objects with label quant_NP\n",
      "\t1058 chunk objects with label quant\n",
      "exporting tf...\n",
      "  0.00s VALIDATING oslots feature\n",
      "  0.12s maxSlot=     426584\n",
      "  0.12s maxNode=    1529088\n",
      "  0.35s OK: oslots is valid\n",
      "   |     0.12s T label                to /Users/cody/github/csl/time_collocations/data/tf\n",
      "   |     0.67s T otype                to /Users/cody/github/csl/time_collocations/data/tf\n",
      "   |     3.02s T oslots               to /Users/cody/github/csl/time_collocations/data/tf\n",
      "   |     0.06s T role                 to /Users/cody/github/csl/time_collocations/data/tf\n",
      "SUCCESS\n"
     ]
    }
   ],
   "source": [
    "chunker.execute()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Composing Time Constructions\n",
    "\n",
    "The current method for isolating phrase heads ([here](https://nbviewer.jupyter.org/github/ETCBC/heads/blob/master/phrase_heads.ipynb)) requires strenuous and ineloquent processing of BHSA subphrase relations. The subphrases are not always consistently encoded and suffer from numerous exceptional cases. The result is that the method is rather convoluted and ineloquent.\n",
    "\n",
    "This notebook will explore the possibility of disconnecting semantic head analysis from the ETCBC subphrase encoding. \n",
    "\n",
    "A \"semantic\" head is the primary content word of a phrase, following Croft's \"Primary Information Bearing Unit\":\n",
    "\n",
    "> **The noun and the verb are the PRIMARY INFORMATION_BEARING UNITS (PIBUs) of the phrase and clause respectively. In common parlance, they are the content words. PIBUs have major informational content that functional elements such as articles and [auxiliaries] do not have. (Croft, *Radical Construction Grammar*, 2001, 258; see also Shead, *Radical Frame Semantics and Biblical Hebrew*, 104)**\n",
    "\n",
    "> **A (semantic) head is the profile equivalent that is the primary information-bearing unit, that is, the most contentful item that most closely profiles the same kind of thing that the whole constituent profiles. (ibid., 259)**\n",
    "\n",
    "Croft also provides an additional criterion to \"profile equivalence\":\n",
    "\n",
    "> **If the criterion of profile equivalence produces two candidates for headhood, the less schematic meaning is the PIBU; that is, the PIBU is the one with the narrower extension, in the formal semantic sense of that term (ibid., 259)**\n",
    "\n",
    "## Inquiry\n",
    "\n",
    "Can we isolate semantic phrase heads in BHSA using only the phrase_atom and phrase limits? This question indeed means that we  take the phrase_atom/phrase boundaries for granted. Empirically, the validity of BHSA phrase boundaries needs to be tested. But for now, the exercise of isolating semantic phrase heads could be seen as the first step towards reproducible phrase boundaries.\n",
    "\n",
    "## Basic Concepts\n",
    "\n",
    "A semantic head will most often stand in a syntactically independent position. For Hebrew nominal phrases, that essentially means a word which is not precided by a construct, and which is semantically central (excluding attributive slots (e.g. H + noun + H + ATTRIBUTIVE) or an adjectival slots (e.g. noun + noun as in אישׁ טוב).\n",
    "\n",
    "Quantifier expressions present unique cases, which may be syntactically independent but semantically secondary. These are expressed through specialized lexical items such as cardinal numbers and qualitative quantifiers (e.g.  \"כל\" and \"חצי\").\n",
    "\n",
    "Another complication is the use of nouns as prepositional items. Such uses can be seen with words like פני \"face\" such as לפני \"in front,\" and even words like ראשׁ as in ראשׁ החדשׁ \"beginning of the month.\" \n",
    "\n",
    "Other expressions of quantity, quality, and function provide similar complexities. These cases have to be specified in advance.\n",
    "\n",
    "### Ambiguity\n",
    "\n",
    "Considerable ambiguity is present in several of cases:\n",
    "\n",
    "**`A B and C`**<br>\n",
    "Given A, B, C == nominal words. Is their relationship `A // B // C` or `A+B // C`. In other words: **what is the relationship of two adjacent nominal words given a list?** Is B a descriptor of A or is it an independent element? \n",
    "\n",
    "**`A of B and C`**<br>\n",
    "Is it, `(A of B) // (C)` or `(A of (B // C)`\n",
    "\n",
    "Or even:\n",
    "\n",
    "**`A of B C and D`**<br>\n",
    "This pattern combines elements from both ambiguous cases.\n",
    "\n",
    "To address these ambiguities we will apply a battery of disambiguation attempts. Some of those attempts will draw from corpus data, i.e. do we ever see `B and C` with the conjunction explicitly elsewhere in the corpus? Or do we ever see a `A of C` excplicitly in the corpus? Accents may also play a role: do we see a conjunctive or disjunctive accent between `B C`? \n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "A number of pre-defined word sets are needed for processing quantification and ambiguous adjacency. These sets are made available in the form of `wsets`, a dictionary containing word sets that are calculated in to the `wordsets` directory of this repository. The following wordsets have been defined:\n",
    "\n",
    "* nominals – a set of word nodes with parts of speech and participles that have the potential to function as nominalized elements. The selected parts of speech are quite permissive: `{'subs', 'nmpr', 'adjv', 'advb', 'prde', 'prps', 'prin', 'inrg'}`. Since parts of speech are not taken as universal linguistic categories but only summaries of language-specific word tendencies (cf. Croft, *Radical Construction Grammar*, 2001), we consider that almost any part of speech can be used in a nominal pattern (or construction). There are some upper limits to this assumption, though. For instance, we exclude cojunctions, articles, prepositions, and negators. \n",
    "* prepositions – a word set consisting of words with a part of speech category of `prep`, a lexical set (`ls`) feature of `ppre` (\"potential preposition\"), as well as a select group of nouns like פני \"face\" which have been processed for prepositionality. \n",
    "* quantifiers - consists of word nodes that are cardinal numbers or qualitative quantifiers such as כל.\n",
    "* mword – mapping from a word to its phonological word group (\"masoretic word\"); joins words on maqqeph and ø space\n",
    "* accent_type – a mapping from a word to its accent type: conjunctive or disjunctive\n",
    "* conj_pairs – a dict of observed conjunction pairings of lexemes in the corpus: `A & B`\n",
    "* cons_pairs – a dict of observed construct pairings of lexemes in the corpus: `A of B`\n",
    "* mom – mapping from word node to its mother word node for a specified relationship: `mom[A]['coord'] = B`\n",
    "* kid – opposite of mom; mapping from word to its children nodes for a relationship: `kid[A]['cons'] = B`\n",
    "\n",
    "**Let's get started**. We load the necessary functions and BHSA data (straight from source)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is Text-Fabric 7.8.12\n",
      "Api reference : https://annotation.github.io/text-fabric/Api/Fabric/\n",
      "\n",
      "123 features found and 6 ignored\n",
      "  0.00s loading features ...\n",
      "   |     0.00s No structure info in otext, the structure part of the T-API cannot be used\n",
      "  7.90s All features loaded/computed - for details use loadLog()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "@font-face {\n",
       "  font-family: \"Ezra SIL\";\n",
       "  src:\n",
       "    local(\"SILEOT.ttf\"),\n",
       "    url(\"https://github.com/annotation/text-fabric/blob/master/tf/server/static/fonts/SILEOT.woff?raw=true\");\n",
       "}\n",
       ".features {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0a6611;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    direction: ltr;\n",
       "}\n",
       ".features div,.features span {\n",
       "    padding: 0;\n",
       "    margin: -0.1rem 0;\n",
       "}\n",
       ".features .f {\n",
       "    font-family: sans-serif;\n",
       "    font-size: x-small;\n",
       "    font-weight: normal;\n",
       "    color: #5555bb;\n",
       "}\n",
       ".features .xft {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-size: medium;\n",
       "  margin: 0.1em 0em;\n",
       "}\n",
       ".features .xft .f {\n",
       "  color: #000000;\n",
       "  background-color: #eeeeee;\n",
       "  font-style: italic;\n",
       "  font-size: small;\n",
       "  font-weight: normal;\n",
       "}\n",
       ".ltr {\n",
       "    direction: ltr ! important;\n",
       "}\n",
       ".verse {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".vl {\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    justify-content: flex-end;\n",
       "    align-items: flex-end;\n",
       "    direction: ltr;\n",
       "    width: 100%;\n",
       "}\n",
       ".outeritem {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    direction: rtl;\n",
       "}\n",
       ".sentence,.clause,.phrase {\n",
       "    margin-top: -1.2em;\n",
       "    margin-left: 1em;\n",
       "    background: #ffffff none repeat scroll 0 0;\n",
       "    padding: 0 0.3em;\n",
       "    border-style: solid;\n",
       "    border-radius: 0.2em;\n",
       "    font-size: small;\n",
       "    display: block;\n",
       "    width: fit-content;\n",
       "    max-width: fit-content;\n",
       "    direction: ltr;\n",
       "}\n",
       ".atoms {\n",
       "    display: flex;\n",
       "    flex-flow: row wrap;\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".satom,.catom,.patom {\n",
       "    margin: 0.3em;\n",
       "    padding: 0.3em;\n",
       "    border-radius: 0.3em;\n",
       "    border-style: solid;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".sentence {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".clause {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".phrase {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 1px;\n",
       "}\n",
       ".satom {\n",
       "    border-color: #aa3333;\n",
       "    border-width: 4px;\n",
       "}\n",
       ".catom {\n",
       "    border-color: #aaaa33;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".patom {\n",
       "    border-color: #33aaaa;\n",
       "    border-width: 3px;\n",
       "}\n",
       ".word {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 1px solid #cccccc;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".lextp {\n",
       "    padding: 0.1em;\n",
       "    margin: 0.1em;\n",
       "    border-radius: 0.1em;\n",
       "    border: 2px solid #888888;\n",
       "    width: fit-content;\n",
       "    display: flex;\n",
       "    flex-flow: column nowrap;\n",
       "    direction: rtl;\n",
       "    background-color: #ffffff;\n",
       "}\n",
       ".occs {\n",
       "    font-size: x-small;\n",
       "}\n",
       ".satom.l,.catom.l,.patom.l {\n",
       "    border-left-style: dotted\n",
       "}\n",
       ".satom.r,.catom.r,.patom.r {\n",
       "    border-right-style: dotted\n",
       "}\n",
       ".satom.lno,.catom.lno,.patom.lno {\n",
       "    border-left-style: none\n",
       "}\n",
       ".satom.rno,.catom.rno,.patom.rno {\n",
       "    border-right-style: none\n",
       "}\n",
       ".tr,.tr a:visited,.tr a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".trb,.trb a:visited,.trb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: normal;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".prb,.prb a:visited,.prb a:link {\n",
       "    font-family: sans-serif;\n",
       "    font-size: large;\n",
       "    direction: ltr;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".h,.h a:visited,.h a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    color: #000044;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".hb,.hb a:visited,.hb a:link {\n",
       "    font-family: \"Ezra SIL\", \"SBL Hebrew\", sans-serif;\n",
       "    font-size: large;\n",
       "    line-height: 2;\n",
       "    direction: rtl;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".vn {\n",
       "  font-size: small !important;\n",
       "  padding-right: 1em;\n",
       "}\n",
       ".rela,.function,.typ {\n",
       "    font-family: monospace;\n",
       "    font-size: small;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".pdp,.pdp a:visited,.pdp a:link {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "    text-decoration: none;\n",
       "}\n",
       ".voc_lex {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vs {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".vt {\n",
       "    font-family: monospace;\n",
       "    font-size: medium;\n",
       "    font-weight: bold;\n",
       "    color: #0000bb;\n",
       "}\n",
       ".gloss {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: normal;\n",
       "    color: #444444;\n",
       "}\n",
       ".vrs {\n",
       "    font-family: sans-serif;\n",
       "    font-size: small;\n",
       "    font-weight: bold;\n",
       "    color: #444444;\n",
       "}\n",
       ".nd {\n",
       "    font-family: monospace;\n",
       "    font-size: x-small;\n",
       "    color: #999999;\n",
       "}\n",
       ".hl {\n",
       "    background-color: #ffee66;\n",
       "}\n",
       "\n",
       "tr.tf, td.tf, th.tf {\n",
       "  text-align: left;\n",
       "}\n",
       "\n",
       "span.hldot {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder: 0.2rem solid var(--hl-rim);\n",
       "\tborder-radius: 0.4rem;\n",
       "\t/*\n",
       "\tdisplay: inline-block;\n",
       "\twidth: 0.8rem;\n",
       "\theight: 0.8rem;\n",
       "\t*/\n",
       "}\n",
       "span.hl {\n",
       "\tbackground-color: var(--hl-strong);\n",
       "\tborder-width: 0;\n",
       "\tborder-radius: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "}\n",
       "\n",
       "span.hlup {\n",
       "\tborder-color: var(--hl-dark);\n",
       "\tborder-width: 0.1rem;\n",
       "\tborder-style: solid;\n",
       "\tborder-radius: 0.2rem;\n",
       "  padding: 0.2rem;\n",
       "}\n",
       "\n",
       ":root {\n",
       "\t--hl-strong:        hsla( 60, 100%,  70%, 0.9  );\n",
       "\t--hl-rim:           hsla( 55, 100%,  60%, 0.9  );\n",
       "\t--hl-dark:          hsla( 55, 100%,  40%, 0.9  );\n",
       "}\n",
       "</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import collections\n",
    "import pickle\n",
    "import random\n",
    "import re\n",
    "import itertools\n",
    "import copy\n",
    "import uuid\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from Levenshtein import distance as lev_dist\n",
    "from IPython.display import display, HTML\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "from tf.app import use\n",
    "from tf.fabric import Fabric\n",
    "from tools.locations import data_locations\n",
    "\n",
    "# load semantic vectors\n",
    "with open('semvector.pickle', 'rb') as infile: \n",
    "    semdist = pickle.load(infile)\n",
    "\n",
    "# load custom BHSA data + heads\n",
    "TF = Fabric(locations=data_locations.values())\n",
    "load_features = ['g_cons_utf8', 'trailer_utf8', 'label', 'lex',\n",
    "                 'role', 'rela', 'typ', 'function', 'language',\n",
    "                 'pdp', 'gloss', 'vs', 'vt', 'nhead', 'head', \n",
    "                 'mother', 'nu', 'prs', 'sem_set', 'ls', 'st',\n",
    "                 'kind', 'top_assoc', 'number', 'obj_prep',\n",
    "                 'embed', 'freq_lex', 'sp']\n",
    "api = TF.load(' '.join(load_features))\n",
    "F, E, T, L = api.F, api.E, api.T, api.L # shortform TF methods\n",
    "\n",
    "A = use('bhsa', api=api, silent=True)\n",
    "A.displaySetup(condenseType='phrase', withNodes=True, extraFeatures='st')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machinery\n",
    "\n",
    "We could use some machinery to do the hard work of looking in and around a node. In the older approach we used TF search templates. But these are not very efficient at scale, and they are always bound by the limits of the query language. I take another approach here: a set of classes that specify locations and directions within a specified context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools.langtools import Positions, PositionsTF, Walker, Dummy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Positions(TF)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Positions` class enables concise access to adjacent nodes within a given context. This allows us to write algorithms with query-like efficiency with all of the power of Python. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This class is instantiated on a word node and can provide contextual look-up data for a given word. For example, given a phrase containing the following word nodes:\n",
    "\n",
    "> (189681, 189682, **189683**, 189684, 189685, 189686) <br>\n",
    "\n",
    "representing the following phrase (space separated for clarity):\n",
    "\n",
    "> ב שׁנת **שׁלשׁים** ו שׁמנה שׁנה\n",
    "\n",
    "Given that the bolded node, `189683` is our `source` word, we instantiate the class, feeding in the node, the \"phrase_atom\" string (which is the context we want to search within), and an instance of Text-Fabric (`tf`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "      #    source node    context  TF instance  \n",
    "      #         |            |       |\n",
    "P = PositionsTF(189683, 'phrase_atom', A).get"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to obtain the word adjacent one space forward, we simply ask `P` for `1`, which gives us the next word in the phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189684"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we try to ask for 4 words forward, we go beyond the bounds of the phrase. But `P` handles this by returning nothing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "P(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To look back one word, we simply give a negative value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189682"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, `P` can be used to quickly call features on these words. For instance, in order to get the lexeme of the word two words in front of `189683`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CMNH/'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2,'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we want to get a number of features, we can just add other features to the arguments. The result is a feature set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'CMNH/', 'sg'}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2, 'lex', 'nu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`P` can also handle features on the source node itself by giving a positionality of `0`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'CLC/'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(0, 'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `Positions` also exists in a non-TF version\n",
    "\n",
    "When the non-tf version of `Positions` is provided any iterable, it can perform the same functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ps = ['The', 'good', 'dog', 'jumped.']\n",
    "\n",
    "P = Positions('good', test_ps).get"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'dog'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Positions can perform a function on the result with an option `do`. In the example below, the word two words ahead is found and an upper-case function is called on the string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'JUMPED.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P(2, do=lambda w: w.upper())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The non-tf version of `Positions` makes it possible to do positionality searches with any ordered list of Python objects that represent linguistic units."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Walker`\n",
    "\n",
    "`Walker` performs a similar function to `Positions`, except it is ambiguous to exact positions, walking either `ahead` or `back` from the source to a target node in the context. A function must be supplied that returns `True` on the target node.\n",
    "\n",
    "We instantiate the `Walker` using the same source and context as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = 189683\n",
    "# get words inside source's phrase_atom\n",
    "positions = L.d(\n",
    "    L.u(189683,'phrase_atom')[0], 'word'\n",
    ")\n",
    "\n",
    "Wk = Walker(source, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Walker` is demonstrated below with the same word. A simple `lambda` function is used to test for the lexeme. In the example below, we find the first word ahead of `189683` that is a cardinal number:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189685"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An alternative demonstrates the `None` returned on the lack of a valid match."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'BOOGABOOGA')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another example wherein we walk backwards to the preposition:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189681"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.back(lambda w: F.sp.v(w) == 'prep')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify that the walk should be interrupted under certain conditions with a `stop` function. In this case we walk forward to the next cardinal number, but the walk is interrupted when the `stop` function detects a conjunction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card',\n",
    "         stop=lambda w: F.sp.v(w) == 'conj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also specify the opposite with a `go` function argument, which defines the nodes that allowed to intervene between `source` and `target`. Below we specify that *only* a conjunction should intervene."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "189685"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk.ahead(lambda w: F.ls.v(w) == 'card',\n",
    "         go=lambda w: F.sp.v(w) == 'conj')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `go` and `stop` functions can be as permissive or strict as desired.\n",
    "\n",
    "Finally, we can tell `Walker` that the output of the validation function should be returned instead of the node itself with the optional argument `output=True`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'card'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_funct = lambda w: F.ls.v(w) if F.ls.v(w)=='card' else None\n",
    "\n",
    "Wk.ahead(val_funct, output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This ability is useful for certain tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like `Positions`, `Walker` can be used in non-TF contexts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ps = ['The', 'bad', 'cat', 'swatted.']\n",
    "\n",
    "Wk_notf = Walker('bad', test_ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swatted.'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk_notf.ahead(lambda w: w.startswith('sw'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Returning All Results along Path\n",
    "\n",
    "`Walker` can also return all results along the path by toggling `every=True`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'swatted.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Wk_notf.ahead(lambda w: type(w)==str, every=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## `Dummy`\n",
    "\n",
    "When writing conditions and logic, we want an object that passively receives `NoneType`s or zero `int`s without throwing errors. Such an object should also return `None` to reflect its `False` value. `Dummy`, provides such functionality. `Dummy` can receive all of the arguments, kwargs, and function calls as a `Positions` or `Walker` object. But it returns absolutely nothing. Ouch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "D = Dummy(None, 'phrase_atom', A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function call below returns `None`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As does this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.get(1, 'lex')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And even this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "D.ahead(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`D` is essentially a souless void that consumes whatever you throw at it and gives nothing in return."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For safe-calls on a `Position` or `Walker` object, assign nodes to it via a function with a `Dummy` given on null nodes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getPos(node, context, tf):\n",
    "    \"\"\"A function to get Positions safely.\"\"\"\n",
    "    if node:\n",
    "        return PositionsTF(node, context, tf)\n",
    "    else:\n",
    "        return Dummy() # <- give dummy on empty node"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "P = getPos(None, 'phrase_atom', A)\n",
    "P.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "P = getPos(1, 'phrase_atom', A)\n",
    "P.get(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Need for Semantic Data\n",
    "\n",
    "The accurate processing of word connections depends on fuller semantic data than BHSA provides. Future semantic data could be stored in a similar way to word sets (`wsets`). \n",
    "\n",
    "For example, in the two phrases\n",
    "\n",
    "> (Exod 25:39) ככר זהב טהור <br>\n",
    "> (2 Sam 24:24) בכסף שקלים חמשׁים\n",
    "\n",
    "we see that זהב and כסף, despite being in two different positions with two different words indicates a kind of \"composed of\" semantic concept: \"round gold\" (i.e. round composed of gold) and \"silver shekels\" (shekels composed of silver). To process these kinds of links, we need a list of nouns that often function as \"material.\" But this is only the beginning. Many other words will have specific semantic values that motivate their syntactic behavior. Such a scope lies outside the bounds of this author's current project on Hebrew time phrases.\n",
    "\n",
    "## A Compromise: Time Phrases\n",
    "\n",
    "Since constructing these semantic classes is vastly time consuming, I want to start with a smaller set of cases. I will instead focus on parsing connections within time phrases for now. This is because I am analyzing time phrases in my current ongoing PhD project. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def disjoint(ph):\n",
    "    \"\"\"Isolate phrases with gaps.\"\"\"\n",
    "    ph = L.d(ph,'word')\n",
    "    for w in ph:\n",
    "        if ph[-1] == w:\n",
    "            break\n",
    "        elif (ph[ph.index(w)+1] - w) > 1:\n",
    "            return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3864 phrases ready\n"
     ]
    }
   ],
   "source": [
    "alltimes = [\n",
    "    ph for ph in F.otype.s('timephrase') \n",
    "]\n",
    "    \n",
    "timephrases = [ph for ph in alltimes if not disjoint(ph)]\n",
    "\n",
    "print(f'{len(timephrases)} phrases ready')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search & Display Functions\n",
    "\n",
    "The functions below allow for fast searching and displaying of queries using a `Construction` object, described in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NB: For the future. Here is a template to plot \n",
    "# a network graph using networkx.\n",
    "\n",
    "# graph = GIVE GRAPH HERE\n",
    "\n",
    "# plt.figure(figsize=(10,5))\n",
    "# pos = nx.drawing.spectral_layout(graph)\n",
    "# nx.draw_networkx(graph, pos)\n",
    "\n",
    "# edge_labels = {\n",
    "#     (n1,n2):graph[n1][n2]['role']\n",
    "#         for n1,n2 in graph.edges\n",
    "# }\n",
    "    \n",
    "# nx.draw_networkx_edge_labels(graph, pos, font_size=10, edge_labels=edge_labels)\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretty(obj, condense='phrase', **kwargs):\n",
    "    \"\"\"Show a linguistic object that is not native to TF app.\"\"\"\n",
    "    index = kwargs.get('index')\n",
    "    kwargs = {k:v for k,v in kwargs.items() if k not in {'index'}}\n",
    "    show = L.d(obj, condense) if index is None else (L.d(obj, condense)[index],)\n",
    "    print(show, not index, index)\n",
    "    A.prettyTuple(show, seq=kwargs.get('seq', obj), **kwargs)\n",
    "\n",
    "def prettyconds(cx):\n",
    "    '''\n",
    "    Iterate through an explain dict for a rela\n",
    "    and print out all of checked conditions.\n",
    "    '''\n",
    "    cx_tree = [\n",
    "        n for n in nx.bfs_tree(cx.graph, cx)\n",
    "            if type(n) == Construction\n",
    "    ]\n",
    "    \n",
    "    for node in cx_tree:\n",
    "        print(f'-- {node} --')\n",
    "        for case in node.cases:\n",
    "            print(f'pattern: {case.get(\"pattern\", case[\"name\"])}')\n",
    "            for cond, value in case['conds'].items():\n",
    "                print('{:<30} {:>30}'.format(cond, str(value)))\n",
    "            print()\n",
    "        \n",
    "def showcx(cx, **kwargs):\n",
    "    \"\"\"Display a construction object with TF.\n",
    "    \n",
    "    Calls TF.show() with HTML highlights for \n",
    "    words/stretch of words that serve a role\n",
    "    within the construction. \n",
    "    \"\"\"\n",
    "    \n",
    "    # get slots for display\n",
    "    refslots = cx.slots if cx.slots else cx.element.slots\n",
    "    showcontext = tuple(set(L.u(s, 'phrase')[0] for s in refslots))\n",
    "    timephrase = L.u(list(refslots)[0], 'timephrase')[0]        \n",
    "\n",
    "    if not cx:\n",
    "        print('NO MATCHES')\n",
    "        print('-'*20)\n",
    "        A.prettyTuple(showcontext, extraFeatures='sp st', withNodes=True, seq=f'{timephrase} -> {cx}')\n",
    "        if kwargs.get('conds'):\n",
    "            prettyconds(cx)\n",
    "        return None\n",
    "\n",
    "    colors = itertools.cycle([\n",
    "        '#96ceb4', '#ffeead', '#ffcc5c', '#ff6f69',\n",
    "        '#bccad6', '#8d9db6', '#667292', '#f1e3dd',\n",
    "    ])\n",
    "    highlights = {}\n",
    "    role2color = {}\n",
    "    \n",
    "    for node in cx.graph.adj[cx]:\n",
    "        role = cx.graph[cx][node]['role']\n",
    "        slots = cx.getslots(node)\n",
    "        color = next(colors)\n",
    "        role2color[role] = color\n",
    "        for slot in slots:\n",
    "            highlights[slot] = color\n",
    "    \n",
    "    A.prettyTuple(\n",
    "        showcontext, \n",
    "        extraFeatures=kwargs.get('extraFeatures', 'sp st lex'), \n",
    "        withNodes=True, \n",
    "        seq=f'{timephrase} -> {cx}', \n",
    "        highlights=highlights\n",
    "    )\n",
    "    # reveal color meanings\n",
    "    for role,color in role2color.items():\n",
    "        colmean = '<div style=\"background: {}; text-align: center\">{}</div>'.format(color, role)\n",
    "        display(HTML(colmean))\n",
    "    \n",
    "    pprint(cx.unfoldroles(), indent=4)\n",
    "    print()\n",
    "    if kwargs.get('conds'):\n",
    "        prettyconds(cx)\n",
    "    display(HTML('<hr>'))\n",
    "        \n",
    "def test_search(\n",
    "    elements, cxtest, \n",
    "    pattern='', \n",
    "    show=None, \n",
    "    end=None, \n",
    "    shuffle=True,\n",
    "    updatei=1000,\n",
    "    select=None,\n",
    "    **kwargs\n",
    "):\n",
    "    '''\n",
    "    Searches phrases with the specified relation \n",
    "    and prints out their descriptive explanation.\n",
    "    '''\n",
    "    \n",
    "    start = datetime.now()\n",
    "    print('beginning search')\n",
    "    \n",
    "    # random shuffle to get good diversity of examples\n",
    "    if shuffle:\n",
    "        random.shuffle(elements)\n",
    "    matches = []\n",
    "    \n",
    "    # iterate and find matches on words\n",
    "    for i,el in enumerate(elements):\n",
    "\n",
    "        # update every 5000 iterations\n",
    "        if i%updatei == 0:\n",
    "            print(f'\\t{len(matches)} found ({i}/{len(elements)})')\n",
    "        \n",
    "        # run test for construction\n",
    "        test = cxtest(el)\n",
    "        \n",
    "        # save results\n",
    "        if test:\n",
    "            if pattern:\n",
    "                if test.pattern == pattern:\n",
    "                    matches.append(test)\n",
    "            else:\n",
    "                matches.append(test)\n",
    "            \n",
    "        # stop at end\n",
    "        if end and len(matches) == end:\n",
    "            break\n",
    "        \n",
    "    # display\n",
    "    print('done at', datetime.now() - start)\n",
    "    print(len(matches), 'matches found...')\n",
    "    if show:\n",
    "        print(f'showing top {show}')\n",
    "    \n",
    "    # option for filtering results\n",
    "    if select:\n",
    "        matches = [m for m in matches if select(m)]\n",
    "        print(f'\\tresults filtered to {len(matches)}')\n",
    "    \n",
    "    for match in matches[:show]:\n",
    "        showcx(match, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construction Classes\n",
    "\n",
    "* `Construction` - an object that represents a linguistic construction; the class records roles and the words that occupy them, as well as has methods for accessing and retrieving data on embedded roles/other constructions\n",
    "* `CXBuilder` - matches conditions to build `Construction` objects; populates them with requisite data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construction2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Construction(object):\n",
    "    \"\"\"A linguistic construction and its attributes.\n",
    "    \n",
    "    This is version 2, which utilizes NetworkX graphs\n",
    "    instead of standard dictionaries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, **specs):\n",
    "        \"\"\"Make a new construction.\n",
    "        \n",
    "        **specs:\n",
    "            name: A name for the construction (CX).\n",
    "            kind: A kind for the CX.\n",
    "            pattern: Name of pattern that matched to license\n",
    "                this CX.\n",
    "            conds: A dictionary of conditions that all eval to\n",
    "                True to license this CX. Keys are strings that\n",
    "                describe what was tested; values are booleans.\n",
    "            cases: A tuple containing all of the possible conds\n",
    "                dicts that were tested and their results, including \n",
    "                non-matches. Useful for debugging.\n",
    "                \n",
    "        Key Attributes:\n",
    "            slots: An ordered tuple of TF slot integers which\n",
    "                describe what span of words in the corpus this\n",
    "                CX represents.\n",
    "            graph: A NetworkX graph object that contains the\n",
    "                internal structure of this cx. Top node of\n",
    "                the graph is this object; edges have values of\n",
    "                \"role\" that give semantic role of each node.\n",
    "            parent: a parent CX if this one is contained in \n",
    "                another's graph.\n",
    "        \"\"\"\n",
    "        \n",
    "        # map optional attributes\n",
    "        for k,v in specs.items():\n",
    "            setattr(self, k, v)\n",
    "            \n",
    "        # map obligatory attributes\n",
    "        self.element = specs.get('element', str(uuid.uuid4()))\n",
    "        self.match = specs.get('match', {})\n",
    "        self.name = specs.get('name', '')\n",
    "        self.kind = specs.get('kind', '')\n",
    "        self.pattern = specs.get('pattern', specs.get('name', ''))\n",
    "        self.conds = specs.get('conds', {})\n",
    "        self.cases = specs.get('cases', tuple())\n",
    "        \n",
    "        # map roles and slots\n",
    "        self.graph = nx.DiGraph()\n",
    "        self.populate_graph(specs.get('roles', {}))\n",
    "        self.slots = tuple()\n",
    "        self.updateslots() # populates self.slots\n",
    "    \n",
    "    def __bool__(self):\n",
    "        \"\"\"Determine truth value of CX.\"\"\"\n",
    "        if self.match:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __repr__(self):\n",
    "        \"\"\"Display CX name with slots.\"\"\"\n",
    "        if self:\n",
    "            return f'CX {self.name} {self.slots}'\n",
    "        else:\n",
    "            return '{CX EMPTY}'\n",
    "        \n",
    "    def _cx_att(self, attr, item):\n",
    "        \"\"\"Get an attribute on a cx or return int\"\"\"\n",
    "        if type(item) == Construction:\n",
    "            return item.__dict__[attr]\n",
    "        elif type(item) == int:\n",
    "            return item\n",
    "            \n",
    "    def _rolestuple(self):\n",
    "        return tuple(\n",
    "            (n1, n2, self.graph[n1][n2]['role'])\n",
    "                 for n1, n2 in nx.bfs_edges(self.graph, self)\n",
    "        )\n",
    "            \n",
    "    def __eq__(self, other):\n",
    "        \"\"\"Determine slot/role-based equality between CXs.\"\"\"\n",
    "        if (\n",
    "            self.__class__ == other.__class__\n",
    "            and self.name == other.name\n",
    "            and str(self._rolestuple) == str(other._rolestuple)\n",
    "        ):\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "        \n",
    "    def __hash__(self):\n",
    "        return hash(\n",
    "            (self.name, self.element)\n",
    "        )\n",
    "    \n",
    "    def __int__(self):\n",
    "        \"\"\"Provide integers for first slot in cx.\n",
    "        \n",
    "        Most relevant for word-level CXs and for\n",
    "        using TF methods on those objects.\n",
    "        \"\"\"\n",
    "        return next(iter(sorted(self.slots)), 0)\n",
    "        \n",
    "    def __contains__(self, cx):\n",
    "        \"\"\"Determine whether certain CX is contained in this one.\"\"\"\n",
    "        return cx in self.subgraph()\n",
    "        \n",
    "    def __deepcopy__(self, memo):\n",
    "        \"\"\"Return a copied version of this CX\"\"\"\n",
    "        roles = {\n",
    "            self.graph[self][node]['role']:node \n",
    "                for node in self.graph.succ[self]\n",
    "        }\n",
    "        attribs = {\n",
    "            k:v for k,v in self.__dict__.items()\n",
    "                if k != 'graph'\n",
    "        }\n",
    "        attribs['roles'] = roles\n",
    "        return Construction(**attribs)\n",
    "        \n",
    "    def getslots(self, item):\n",
    "        \"\"\"Get TF integer slots as tuple.\"\"\"\n",
    "        slots = self._cx_att('slots', item)\n",
    "        if type(slots) == tuple:\n",
    "            return slots\n",
    "        else:\n",
    "            return (slots,)\n",
    "            \n",
    "    def populate_graph(self, rolesdict):\n",
    "        \"\"\"Populate the graph with the CX's structure\"\"\"\n",
    "        \n",
    "        # populate graph with roles\n",
    "        self.graph.add_node(self)\n",
    "        for role, child in rolesdict.items():\n",
    "            \n",
    "            # create unique copy of child \n",
    "            # esp. relevant for CX objects\n",
    "            # that are shared between other CXs\n",
    "            child = copy.deepcopy(child)\n",
    "            \n",
    "            # add child to graph\n",
    "            self.graph.add_edge(self, child, role=role)\n",
    "            \n",
    "            # import child's graph structure\n",
    "            if type(child) == Construction:\n",
    "                self.graph.update(child.graph)\n",
    "                child.graph = self.graph # assign graph to child\n",
    "    \n",
    "    def subgraph(self):\n",
    "        \"\"\"Return graph governed by this CX\"\"\"        \n",
    "        # return subgraph\n",
    "        return self.graph.subgraph(nx.bfs_tree(self.graph, self))\n",
    "    \n",
    "    def updategraph(self, oldnode, newnode):\n",
    "        \"\"\"Update the internal structure of CX graph.\n",
    "        \n",
    "        Change left to right.\n",
    "        \"\"\"\n",
    "        \n",
    "        # get predecessor for reassignment\n",
    "        pred = next(iter(self.graph.pred[oldnode]))\n",
    "        \n",
    "        # get replacement role \n",
    "        role = self.graph[pred][oldnode]['role']\n",
    "\n",
    "        # remove old node\n",
    "        self.graph.remove_node(oldnode)\n",
    "\n",
    "        # make unique copy of newnode\n",
    "        newnode = copy.deepcopy(newnode)\n",
    "\n",
    "        # add new node\n",
    "        self.graph.add_edge(pred, newnode, role=role)\n",
    "\n",
    "        # add new nodes's constituents & roles to graph\n",
    "        if type(newnode) == Construction:\n",
    "            self.graph.update(newnode.graph)\n",
    "            newnode.graph = self.graph # assign graph to child\n",
    "            \n",
    "        # remap slots to reflect new nodes\n",
    "        self.updateslots()\n",
    "        \n",
    "        # remap slots for constituent cxs\n",
    "        for node in self.graph:\n",
    "            if type(node) == Construction:\n",
    "                node.updateslots()\n",
    "        \n",
    "    def updateslots(self):\n",
    "        \"\"\"Update the slots list.\"\"\"\n",
    "        self.slots = tuple(sorted(set(\n",
    "            slot for node in nx.bfs_tree(self.graph, self)\n",
    "                for slot in self.getslots(node)\n",
    "        )))\n",
    "        \n",
    "    def getrole(self, role, default=None):\n",
    "        \"\"\"Retrieves the adjacent node of a specific role.\n",
    "        \n",
    "        If node is not present, return default.\n",
    "        \"\"\"\n",
    "        for node in self.graph.succ[self]:\n",
    "            if self.graph[self][node]['role'] == role:\n",
    "                return node\n",
    "        return default\n",
    "    \n",
    "    def getsuccroles(self, role, start=None):\n",
    "        \"\"\"Retrieve successive roles.\n",
    "        \n",
    "        Recursively calls down the graph looking\n",
    "        for successive roles.\n",
    "        E.g. \n",
    "        >    head -> head -> head\n",
    "        but not\n",
    "        >    head -> adjv -> head\n",
    "        \"\"\"\n",
    "        start = start or self\n",
    "        for adj_node in self.graph.adj[start]:\n",
    "            if self.graph[start][adj_node]['role'] == role:\n",
    "                yield adj_node\n",
    "                yield from self.getsuccroles(role, start=adj_node)\n",
    "                \n",
    "    def unfoldroles(self, cx=None):\n",
    "        \"\"\"Return all contained construction roles as a dict.\n",
    "\n",
    "        Recursively calls down into graph nodes to populate\n",
    "        a recursive dict along with labels.\n",
    "        \"\"\"\n",
    "        cx = cx if cx is not None else self\n",
    "        roledict = {}\n",
    "        roledict['__cx__'] = cx.name\n",
    "        for child in self.graph.succ[cx]:\n",
    "            role = self.graph[cx][child]['role']\n",
    "            if type(child) == Construction:\n",
    "                roledict[role] = self.unfoldroles(child)\n",
    "            elif type(child) == int:\n",
    "                roledict[role] = child\n",
    "        return roledict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(range(1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CXbuilder2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Debugger(object):\n",
    "    \"\"\"Display debugging messages if toggled\"\"\"\n",
    "    def __init__(self, boolean):\n",
    "        self.report = boolean\n",
    "        self.indent = 0\n",
    "    def say(self,msg, end='\\n', **kwargs):\n",
    "        self.indent = kwargs.get('indent', self.indent)\n",
    "        if self.report:\n",
    "            indent = self.indent * '\\t'\n",
    "            fmtmsg = f'{indent}{msg}{end}'\n",
    "            sys.stderr.write(fmtmsg)\n",
    "\n",
    "class CXbuilder(object):\n",
    "    \"\"\"Identifies and builds constructions using Text-Fabric nodes.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize CXbuilder, giving methods for CX detection.\"\"\"\n",
    "        \n",
    "        # cache matched constructions for backreferences\n",
    "        self.cache = collections.defaultdict(\n",
    "            lambda: collections.defaultdict()\n",
    "        )\n",
    "        \n",
    "        # NB: objects below should be overwritten \n",
    "        # and configured for the particular cxs needed\n",
    "        self.cxs = tuple()\n",
    "        self.yieldsto = {} \n",
    "        \n",
    "        # for drip-bucket categories\n",
    "        self.dripbucket = tuple()\n",
    "    \n",
    "    def cxcache(self, element, name, method):\n",
    "        \"\"\"Get cx from cache or run.\"\"\"\n",
    "        try:\n",
    "            return self.cache[element][name]\n",
    "        except KeyError:\n",
    "            return method(element)\n",
    "    \n",
    "    def test(self, *cases):\n",
    "        \"\"\"Populate Construction obj based on a cases's all Truth value.\n",
    "        \n",
    "        The last-matching case will be used to populate\n",
    "        a Construction object. This allows more complex\n",
    "        cases to take precedence over simpler ones.\n",
    "        \n",
    "        Args:\n",
    "            cases: an arbitrary number of dictionaries,\n",
    "                each of which contains a string key that\n",
    "                describes the test and a test that evals \n",
    "                to a Boolean.\n",
    "        \n",
    "        Returns:\n",
    "            a populated or blank Construction object\n",
    "        \"\"\"\n",
    "        \n",
    "        # find cases where all cnds == True\n",
    "        test = [\n",
    "            case for case in cases\n",
    "                if all(case['conds'].values())\n",
    "                    and all(case['roles'].values())\n",
    "        ]\n",
    "        \n",
    "        # return last test\n",
    "        if test:\n",
    "            cx = Construction(\n",
    "                match=test[-1],\n",
    "                cases=cases,\n",
    "                **test[-1]\n",
    "            )\n",
    "            self.cache[cx.element][cx.name] = cx\n",
    "            return cx\n",
    "        else:\n",
    "            return Construction(cases=cases, **cases[0])\n",
    "        \n",
    "    def findall(self, element):\n",
    "        \"\"\"Runs analysis for all constructions with an element.\n",
    "        \n",
    "        Returns as dict with test:result as key:value.\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        # add cxs from this builder\n",
    "        for funct in self.cxs:\n",
    "            cx = funct(element)\n",
    "            if cx:\n",
    "                results.append(cx)\n",
    "        \n",
    "        # apply drip-bucket categories\n",
    "        if not results:\n",
    "            for funct in self.dripbucket:\n",
    "                cx = funct(element)\n",
    "                if cx:\n",
    "                    results.append(cx)\n",
    "        \n",
    "        return results\n",
    "                        \n",
    "    def sortbyslot(self, cxlist):\n",
    "        \"\"\"Sort constructions by order of contained slots.\"\"\"\n",
    "        sort = sorted(\n",
    "            ((sorted(cx.slots), cx) for cx in cxlist),\n",
    "            key=lambda k: k[0]\n",
    "        )\n",
    "        return [cx[-1] for cx in sort]\n",
    "    \n",
    "    def clusterCXs(self, cxlist):\n",
    "        \"\"\"Cluster constructions which overlap in their slots/roles.\n",
    "\n",
    "        Overlapping constructions form a graph wherein the constructions \n",
    "        are nodes and the overlaps are edges. This algorithm retrieves all \n",
    "        interconnected constructions. It does so with a recursive check \n",
    "        for overlapping slot sets. Merging the slot sets produces new \n",
    "        overlaps. The algorithm passes over all constructions until no \n",
    "        further overlaps are detected.\n",
    "\n",
    "        Args:\n",
    "            cxlist: list of Construction objects\n",
    "\n",
    "        Returns:\n",
    "            list of lists, where each embedded list \n",
    "            is a cluster of overlapping constructions.\n",
    "        \"\"\"\n",
    "\n",
    "        clusters = []\n",
    "        cxlist = [i for i in cxlist] # operate on copy\n",
    "\n",
    "        # iterate until no more intersections found\n",
    "        thiscluster = [cxlist.pop(0)]\n",
    "        theseslots = set(s for s in thiscluster[0].slots)\n",
    "\n",
    "        # loop continues as it snowballs and picks up slots\n",
    "        # loop stops when a complete loop produces no other matches\n",
    "        while cxlist:\n",
    "\n",
    "            matched = False # whether loop was successful\n",
    "\n",
    "            for cx in cxlist:\n",
    "                if theseslots & set(cx.slots):\n",
    "                    thiscluster.append(cx)\n",
    "                    theseslots |= set(cx.slots)\n",
    "                    matched = True\n",
    "\n",
    "            # cxlist shrinks; when empty, it stops loop\n",
    "            cxlist = [\n",
    "                cx for cx in cxlist \n",
    "                    if cx not in thiscluster\n",
    "            ]\n",
    "\n",
    "            # assemble loop\n",
    "            if not matched:\n",
    "                clusters.append(thiscluster)\n",
    "                thiscluster = [cxlist.pop(0)]\n",
    "                theseslots = set(s for s in thiscluster[0].slots)\n",
    "        \n",
    "        # add last cluster\n",
    "        clusters.append(thiscluster)\n",
    "\n",
    "        return clusters\n",
    "\n",
    "    def test_yield(self, cx1, cx2):\n",
    "        \"\"\"Determine whether to submit cx1 to cx2.\"\"\"\n",
    "        \n",
    "        # get name or class yields\n",
    "        cx1yields = self.yieldsto.get(\n",
    "            cx1.name,\n",
    "            self.yieldsto.get(cx1.kind, set())\n",
    "        )\n",
    "        # test yields\n",
    "        if type(cx1yields) == set:\n",
    "            return bool({cx2.name, cx2.kind} & cx1yields)\n",
    "        elif type(cx1yields) == bool:\n",
    "            return cx1yields\n",
    "        \n",
    "    def interslots(self, cx1, cx2):\n",
    "        \"\"\"Get the intersecting slots of two CXs\n",
    "        \n",
    "        Return as sorted tuple.\n",
    "        \"\"\"\n",
    "        return tuple(sorted(\n",
    "            set(cx1.slots) & set(cx2.slots)\n",
    "        ))\n",
    "    \n",
    "    def slots2node(self, cx, slots):\n",
    "        \"\"\"Get a CX node from a tuple of slots.\"\"\"\n",
    "        for node in nx.bfs_tree(cx.graph, cx):\n",
    "            if cx.getslots(node) == slots:\n",
    "                return node\n",
    "    \n",
    "    def intersect_node(self, cx1, cx2):\n",
    "        \"\"\"Get node from cx1 with slots common with cx2.\"\"\"\n",
    "        intersect = self.interslots(cx1, cx2)\n",
    "        return self.slots2node(cx1, intersect)\n",
    "\n",
    "    def weaveCX(self, cxlist, debug=False):\n",
    "        \"\"\"Weave together constructions on their intersections.\n",
    "\n",
    "        Overlapping constructions form a graph wherein constructions \n",
    "        are nodes and the overlaps are edges. The graph indicates\n",
    "        that the constructions function together as one single unit.\n",
    "        weaveCX combines all constructions into a single one. Moving\n",
    "        from right-to-left (Hebrew), the function consumes and subsumes\n",
    "        subsequent constructions to previous ones. The result is a \n",
    "        single unit with embedding based on the order of consumption.\n",
    "        Roles in previous constructions are thus expanded into the \n",
    "        constructions of their subsequent constituents.\n",
    "        \n",
    "        For instance, take the following phrase in English:\n",
    "        \n",
    "            >    \"to the dog\"\n",
    "            \n",
    "        Say a CXbuilder object contains basic noun patterns and can\n",
    "        recognize the following contained constructions:\n",
    "        \n",
    "            >    cx Preposition: ('prep', to), ('obj', the),\n",
    "            >    cx Definite: ('art', the), ('noun', dog)\n",
    "        \n",
    "        When the words of the constructions are compared, an overlap\n",
    "        can be seen:\n",
    "        \n",
    "            >    cx Preposition:    to  the\n",
    "            >    cx Definite:           the  dog\n",
    "        \n",
    "        The overlap in this case is \"the\". The overlap suggests that\n",
    "        the slot filled by \"the\" in the Preposition construction \n",
    "        should be expanded. This can be done by remapping the role\n",
    "        filled by \"the\" alone to the subsequent Definite construction.\n",
    "        This results in embedding:\n",
    "        \n",
    "            >    cx Preposition: ('prep', to), \n",
    "                                 ('obj', cx Definite: ('art', the), \n",
    "                                                      ('noun', dog))\n",
    "        \n",
    "        weaveCX accomplishes this by calling the updaterole method native\n",
    "        to Construction objects. The end result is a list of merged \n",
    "        constructions that contain embedding.\n",
    "        \n",
    "        Args: \n",
    "            cxlist: a list of constructions pre-sorted for word order;\n",
    "                the list shrinks throughout recursive iteration until\n",
    "                the job is finished\n",
    "            cx: a construction object to begin/continue analysis on\n",
    "            debug: an option to display debugging messages for when \n",
    "                things go wrong 🤪\n",
    "                \n",
    "        Prerequisites:\n",
    "            self.yieldsto: A dictionary in CXbuilder that tells weaveCX\n",
    "                to subsume one construction into another regardless of\n",
    "                word order. Key is name of submissive construction, value\n",
    "                is a set of dominating constructions. Important for, e.g., \n",
    "                cases of quantification where a head-noun might be preceded \n",
    "                by a chain of quantifiers but should still be at the top of \n",
    "                the structure since it is more semantically prominent.\n",
    "                \n",
    "        Returns:\n",
    "            a list of composed constructions\n",
    "        \"\"\"\n",
    "        \n",
    "        db = Debugger(debug)\n",
    "        \n",
    "        db.say(f'\\nReceived cxlist {cxlist}', indent=0)\n",
    "\n",
    "        # compile all cxs to here\n",
    "        root = copy.deepcopy(cxlist.pop(0))\n",
    "        \n",
    "        db.say(f'Beginning analysis with {root}')\n",
    "        \n",
    "        # begin matching and remapping\n",
    "        while cxlist:\n",
    "            \n",
    "            # get next cx\n",
    "            ncx = copy.deepcopy(cxlist.pop(0))\n",
    "            \n",
    "            # find root node with slots intersecting next cx\n",
    "            db.say(f'comparing {root} with {ncx}', indent=1)\n",
    "            node = self.intersect_node(root, ncx)\n",
    "            db.say(f'intersect is at {node}')\n",
    "            \n",
    "            # remove cxs covered by larger version\n",
    "            if root in ncx:\n",
    "                db.say(f'root {root} in ncx {ncx}...replacing root with ncx')\n",
    "                root = ncx\n",
    "            \n",
    "            # update yielded nodes\n",
    "            elif self.test_yield(node, ncx):\n",
    "                \n",
    "                db.say(f'{node} being yielded to {ncx}')\n",
    "                   \n",
    "                # get top-most yielding node\n",
    "                path = nx.shortest_path(root.graph, root, node)\n",
    "                while path and self.test_yield(path[-1], ncx):\n",
    "                    node = path.pop(-1)\n",
    "                \n",
    "                db.say(f'top-yielding node is {node}', indent=2)\n",
    "                   \n",
    "                # update ncx graph\n",
    "                db.say(f'comparing {ncx} with {node}')\n",
    "                ncxnode = self.intersect_node(ncx, node)\n",
    "                db.say(f'intersect is at {ncxnode}')\n",
    "                ncx.updategraph(ncxnode, node)\n",
    "                db.say(f'ncx updated to {ncx}')\n",
    "                \n",
    "                # update root graph or remap root to ncx\n",
    "                if root != node:\n",
    "                    rnode = self.intersect_node(root, ncx)\n",
    "                    db.say(f'replacing node {rnode} in root {root} with {ncx}')\n",
    "                    root.updategraph(rnode, ncx)\n",
    "                    \n",
    "                else:\n",
    "                    # switch root and ncx\n",
    "                    db.say(f'switching {root} with {ncx}')\n",
    "                    root = ncx\n",
    "                 \n",
    "            # update all non-yielding nodes\n",
    "            else:\n",
    "                db.say(f'\\tupdating {node} in root with {ncx}')\n",
    "                root.updategraph(node, ncx)\n",
    "            \n",
    "        return root\n",
    "            \n",
    "    def analyzestretch(self, stretch, debug=False):\n",
    "        \"\"\"Analyze an entire stretch of a linguistic unit.\n",
    "        \n",
    "        Applies construction tests for every constituent \n",
    "        and merges all overlapping constructions into a \n",
    "        single construction.\n",
    "        \n",
    "        Args:\n",
    "            stretch: an iterable containing elements that\n",
    "                are tested by construction tests to build\n",
    "                Construction objects. e.g. stretch might be \n",
    "                a list of TF word nodes.\n",
    "            debug: option to display debuggin messages\n",
    "        \n",
    "        Returns:\n",
    "            list of merged constructions\n",
    "        \"\"\"\n",
    "                   \n",
    "        db = Debugger(debug)\n",
    "        \n",
    "        # match elements to constructions based on tests\n",
    "        rawcxs = [\n",
    "            match for element in stretch\n",
    "                for match in self.findall(element)\n",
    "                    if match\n",
    "        ]\n",
    "        \n",
    "        db.say(f'rawcxs found: {rawcxs}...')\n",
    "        \n",
    "        # return empty results\n",
    "        if not rawcxs:\n",
    "            db.say(f'!no cx pattern matches! returning []')\n",
    "            return []\n",
    "            \n",
    "        # cluster and sort matched constructions\n",
    "        clsort = [\n",
    "            self.sortbyslot(cxlist)\n",
    "                for cxlist in self.clusterCXs(rawcxs)    \n",
    "        ]\n",
    "    \n",
    "        db.say(f'cxs clustered into: {clsort}...')\n",
    "    \n",
    "        db.say(f'Beginning weaveCX method...')\n",
    "        # merge overlapping constructions\n",
    "        cxs = [\n",
    "            self.weaveCX(cluster, debug=debug)\n",
    "                for cluster in clsort\n",
    "        ]\n",
    "        \n",
    "        return cxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CXbuilder with Text-Fabric Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXbuilderTF(CXbuilder):\n",
    "    \"\"\"Build Constructions with TF integration.\"\"\"\n",
    "    \n",
    "    def __init__(self, tf, **kwargs):\n",
    "        \n",
    "        # set up TF data for tests\n",
    "        self.tf = tf\n",
    "        self.F, self.T, self.L = tf.api.F, tf.api.T, tf.api.L\n",
    "        self.context = kwargs.get('context', 'timephrase')\n",
    "        \n",
    "        # set up CXbuilder\n",
    "        CXbuilder.__init__(self)\n",
    "\n",
    "    def getP(self, node):\n",
    "        \"\"\"Get Positions object for a TF node.\n",
    "        \n",
    "        Return Dummy object if not node.\n",
    "        \"\"\"\n",
    "        if not node:\n",
    "            return Dummy()\n",
    "        return PositionsTF(node, self.context, self.tf).get\n",
    "    \n",
    "    def getWk(self, node):\n",
    "        \"\"\"Get Walker object for a TF word node.\n",
    "        \n",
    "        Return Dummy object if not node.\n",
    "        \"\"\"\n",
    "        if not node:\n",
    "            return Dummy()\n",
    "        \n",
    "        # format tf things to send\n",
    "        thisotype = self.F.otype.v(node)\n",
    "        context = self.L.u(node, self.context)[0]\n",
    "        positions = self.L.d(context, thisotype)        \n",
    "        return Walker(node, positions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Constructions\n",
    "\n",
    "The `wordConstructions` builder class recognizes word semantic classes and types based on provided criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wordConstructions(CXbuilderTF):\n",
    "    \"\"\"Build word constructions.\"\"\"\n",
    "    \n",
    "    def __init__(self, tf, **kwargs):\n",
    "        \n",
    "        \"\"\"Initialize with Constructions attribs/methods.\"\"\"\n",
    "        CXbuilderTF.__init__(self, tf, **kwargs)\n",
    "        \n",
    "        # Order matters! More specific meanings last\n",
    "        self.cxs = (\n",
    "            self.prep,\n",
    "            self.qual_quant,\n",
    "            self.card,\n",
    "            self.ordn,\n",
    "            self.name,\n",
    "            self.cont_ptcp,\n",
    "        )\n",
    "        \n",
    "        self.dripbucket = (\n",
    "            self.pos,\n",
    "        )\n",
    "        \n",
    "        self.kind = 'word_cx'\n",
    "    \n",
    "    def cxdict(self, slotlist):\n",
    "        \"\"\"Map all TF word slots to a construction.\n",
    "        \n",
    "        Method returns a dictionary of slot:cx\n",
    "        mappings.\n",
    "        \"\"\"\n",
    "        \n",
    "        slot2cx = {}\n",
    "        for w in slotlist:\n",
    "            for cx in self.findall(w):\n",
    "                slot2cx[w] = cx\n",
    "    \n",
    "        return slot2cx\n",
    "    \n",
    "    def pos(self, w):\n",
    "        \"\"\"A drip-bucket part of speech CX.\n",
    "        \n",
    "        The standard ETCBC feature is pdp,\n",
    "        which is \"phrase-dependent part of\n",
    "        speech.\" I.e. it is a contextually\n",
    "        sensive pos label.\n",
    "        \"\"\"\n",
    "        \n",
    "        F = self.F\n",
    "        \n",
    "        # map\n",
    "        pdplabel = {\n",
    "            'subs': 'cont',\n",
    "            'adjv': 'cont',\n",
    "            'advb': 'cont',\n",
    "        }\n",
    "        pdp = F.pdp.v(w)\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': f'{pdplabel.get(pdp, pdp)}',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'head': w},\n",
    "                'conds': {\n",
    "                    f'bool(F.pdp.v({w}))':\n",
    "                        bool(F.pdp.v(w)),\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def prep(self, w):\n",
    "        \"\"\"A preposition word.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        F = self.F\n",
    "        name = 'prep'\n",
    "        roles = {'head': w}\n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'ETCBC pdp',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.pdp.v(w) == prep':\n",
    "                        F.pdp.v(w) == 'prep',\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'ETCBC ppre words',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.ls.v(w) == ppre':\n",
    "                        F.ls.v(w) == 'ppre',\n",
    "                    'F.lex.v(w) != DRK/':\n",
    "                        F.lex.v(w) != 'DRK/',\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'R>C/',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.lex.v(w) == R>C/':\n",
    "                        F.lex.v(w) == 'R>C/',\n",
    "                    'F.st.v(w) == c':\n",
    "                        F.st.v(w) == 'c',\n",
    "                    'P(-1,pdp) == prep':\n",
    "                        P(-1,'pdp') == 'prep',\n",
    "                    'phrase is adverbial':\n",
    "                        F.function.v(\n",
    "                            L.u(w,'phrase')[0]\n",
    "                        ) in {\n",
    "                            'Time', 'Adju', \n",
    "                            'Cmpl', 'Loca',\n",
    "                        },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'construct lexs',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.lex.v(w) in lexset':\n",
    "                        F.lex.v(w) in {\n",
    "                            'PNH/','TWK/', \n",
    "                            'QY/', 'QYH=/', \n",
    "                            'QYT/', '<WD/'\n",
    "                        },\n",
    "                    'F.prs.v(w) == absent':\n",
    "                        F.prs.v(w) == 'absent',\n",
    "                    'F.st.v(w) == c':\n",
    "                        F.st.v(w) == 'c'\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'L+BD',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.lex.v(w) == BD/':\n",
    "                        F.lex.v(w) == 'BD/',\n",
    "                    'P(-1,lex) == L':\n",
    "                        P(-1,'lex') == 'L',\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': '>XRJT/',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.lex.v(w) == >XRJT/':\n",
    "                        F.lex.v(w) == '>XRJT/',\n",
    "                    'F.st.v(w) == c':\n",
    "                        F.st.v(w) == 'c',\n",
    "                    'P(1,lex) or P(2,lex) not >JWB|RC</':\n",
    "                        not {\n",
    "                            P(1,'lex'), P(2,'lex')\n",
    "                        } & {\n",
    "                            '>JWB/', 'RC</'\n",
    "                        }\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': '<YM/ time',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.lex.v(w) == <YM/':\n",
    "                        F.lex.v(w) == '<YM/',\n",
    "                    'F.st.v(w) == c':\n",
    "                        F.st.v(w) == 'c',\n",
    "                    'F.function.v(phrase) == Time':\n",
    "                        F.function.v(\n",
    "                            L.u(w,'phrase')[0]\n",
    "                        ) == 'Time',\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def name(self, w):\n",
    "        \"\"\"A name word (i.e. proper noun).\"\"\"\n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'name',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'head': w},\n",
    "                'conds': {\n",
    "                    'F.pdp.v(w) == nmpr':\n",
    "                        self.F.pdp.v(w) == 'nmpr'\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def cont_ptcp(self, w):\n",
    "        \"\"\"A content word participle.\n",
    "        \n",
    "        A participle which can potentially\n",
    "        function like a \"noun\" i.e. a content word.\n",
    "        \"\"\"\n",
    "        \n",
    "        F = self.F\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'cont',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'participle',\n",
    "                'roles': {'head': w},\n",
    "                'conds': {\n",
    "                    'F.sp.v(w) == verb':\n",
    "                        F.sp.v(w) == 'verb',\n",
    "                    'F.vt.v(w) in {ptcp, ptca}':\n",
    "                        F.vt.v(w) in {'ptcp', 'ptca'},\n",
    "                }\n",
    "            },\n",
    "        )    \n",
    "    \n",
    "    def card(self, w):\n",
    "        \"\"\"A cardinal number.\"\"\"\n",
    "        \n",
    "        F = self.F\n",
    "        P = self.getP(w)\n",
    "        name = 'card'\n",
    "        roles = {'head': w}\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.ls.v(w) == card':\n",
    "                        F.ls.v(w) == 'card',\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    def ordn(self, w):\n",
    "        \"\"\"An ordinal word.\"\"\"\n",
    "        \n",
    "        F = self.F\n",
    "        P = self.getP(w)\n",
    "        roles = {'head': w}\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'ordn',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'ETCBC ls',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    'F.ls.v(w) == ordn':\n",
    "                        F.ls.v(w) == 'ordn',\n",
    "                }\n",
    "            },\n",
    "        )\n",
    "    \n",
    "    def qual_quant(self, w):\n",
    "        \"\"\"A qualitative quantifier word.\"\"\"\n",
    "        \n",
    "        F = self.F\n",
    "        P = self.getP(w)\n",
    "        name = 'qquant'\n",
    "        roles = {'head': w}\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'qualitative',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    f'{F.lex.v(w)} in lexset':\n",
    "                        F.lex.v(w) in {\n",
    "                            'KL/', 'M<V/', 'JTR/',\n",
    "                            'XYJ/', 'C>R=/', 'MSPR/', \n",
    "                            'RB/', 'RB=/',\n",
    "                        },\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'portion',\n",
    "                'roles': roles,\n",
    "                'conds': {\n",
    "                    f'{F.lex.v(w)} in lexset':\n",
    "                        F.lex.v(w) in {\n",
    "                            'M<FR/', '<FRWN/',\n",
    "                            'XMJCJT/',\n",
    "                        },\n",
    "                }\n",
    "            },\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## \"TP\" Constructions\n",
    "\n",
    "The `TPConstructions` class prepares Time Phrase constructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TPConstructions(CXbuilderTF):\n",
    "    \"\"\"Class for building time phrase constructions.\"\"\"\n",
    "    \n",
    "    def __init__(self, wordcxs, tf, **kwargs):\n",
    "        \n",
    "        \"\"\"Initialize with Constructions attribs/methods.\"\"\"\n",
    "        CXbuilderTF.__init__(self, tf, **kwargs)\n",
    "        \n",
    "        self.words = wordcxs\n",
    "        \n",
    "        # map cx searches for full analyses\n",
    "        self.cxs = (\n",
    "            self.defi,\n",
    "            self.card_chain,\n",
    "            self.demon,\n",
    "            self.adjv,\n",
    "            self.advb,\n",
    "            self.attrib,\n",
    "            self.geni,\n",
    "            self.numb,\n",
    "            self.prep,\n",
    "        )\n",
    "        \n",
    "        self.dripbucket = (\n",
    "            self.wordphrase,\n",
    "        )\n",
    "        \n",
    "        self.kind = 'TP_construction'\n",
    "        \n",
    "        # submit these cxs to cx in set \n",
    "        self.yieldsto = {\n",
    "            'card_chain': {'numb_ph'},\n",
    "            'word_cx': {self.kind}\n",
    "        }\n",
    "        \n",
    "    def word(self, w):\n",
    "        \"\"\"Safely get word CX\"\"\"\n",
    "        return self.words.get(w, Construction())\n",
    "        \n",
    "    def wordphrase(self, w):\n",
    "        \"\"\"A phrase construction for one word.\n",
    "        \n",
    "        Returns first matching word cx for a word.\n",
    "        \"\"\"\n",
    "        return self.word(w)\n",
    "        \n",
    "    def defi(self, w):\n",
    "        \"\"\"Matches a definite construction.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        \n",
    "        return self.test( \n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'defi_ph',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'art': self.word(w), 'head': self.word(P(1))},\n",
    "                'conds': {\n",
    "\n",
    "                    f'F.sp.v({w}) == art':\n",
    "                        self.F.sp.v(w) == 'art',\n",
    "\n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1))\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def prep(self, w):\n",
    "        \"\"\"Matches a preposition with a modified element.\"\"\"\n",
    "                \n",
    "        P = self.getP(w)\n",
    "        Wk =  self.getWk(w)\n",
    "        F = self.F\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'prep_ph',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'prep':self.word(w), 'head':self.word(P(1))},\n",
    "                'conds': {\n",
    "\n",
    "                    f'({w}).name == prep':\n",
    "                        self.word(w).name == 'prep',\n",
    "\n",
    "                    f'F.prs.v({w}) == absent':\n",
    "                        self.F.prs.v(w) == 'absent',\n",
    "                    \n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1)),\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'prep_ph',\n",
    "                'pattern': 'suffix',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'prep': self.word(w), 'head': self.word(w)},\n",
    "                'conds': {\n",
    "                    \n",
    "                    f'({w}).name == prep':\n",
    "                        self.word(w).name == 'prep',\n",
    "                    \n",
    "                    'F.prs.v(w) not in {absent, NA}':\n",
    "                        F.prs.v(w) not in {'absent', 'NA'},\n",
    "                }\n",
    "                \n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'prep_ph',\n",
    "                'pattern': 'prep...on',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'prep': self.word(w), 'head': self.word(w)},\n",
    "                'conds': {\n",
    "                    f'{F.lex.v(w)} in lexset':\n",
    "                        F.lex.v(w) in {'M<L/', 'HL>H'},\n",
    "                    f'Wk.back(({w}).name == prep)':\n",
    "                        bool(Wk.back(lambda n: self.word(n).name=='prep'))\n",
    "                }\n",
    "                \n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def geni(self, w):\n",
    "        \"\"\"Queries for \"genitive\" relations on a word.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        word = self.word\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'geni_ph',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'geni': self.word(w), 'head': self.word(P(-1))},\n",
    "                'conds': {\n",
    "\n",
    "                    'P(-1, st) == c': \n",
    "                        P(-1,'st') == 'c',\n",
    "\n",
    "                    'P(-1).name not in {qquant,card}':\n",
    "                        word(P(-1)).name not in {'qquant','card'},\n",
    "                    \n",
    "                    'P(-1).name != prep':\n",
    "                        word(P(-1)).name != 'prep',\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "\n",
    "    def advb(self, w):\n",
    "        \"\"\"Match and adverb and its mod.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        word = self.word\n",
    "        \n",
    "        return self.test(\n",
    "           {\n",
    "                'element': w,\n",
    "                'name': 'advb_ph',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'advb': word(w), 'head': word(P(1))},\n",
    "                'conds': {\n",
    "                    f'F.sp.v({w}) == advb':\n",
    "                        self.F.sp.v(w) == 'advb',\n",
    "                    'P(-1,sp) != art':\n",
    "                        P(-1,'sp') != 'art',\n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1)),\n",
    "                    'P(1,sp) != conj': # ensure not a nominal use\n",
    "                        P(1,'sp') != 'conj',\n",
    "                    'P(-1).name != prep': # ensure not nominal\n",
    "                        word(P(-1)).name != 'prep',\n",
    "                    f'F.lex.v({F.lex.v(w)}) not in noadvb_set':\n",
    "                        F.lex.v(w) not in {'JWMM'},\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def adjv(self, w):\n",
    "        \"\"\"Matches a word serving as an adjective.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        F = self.F\n",
    "        word = self.word\n",
    "        name = 'adjv_ph'\n",
    "        \n",
    "        # check for recursive adjective matches \n",
    "        a2match = self.adjv(P(-1)) if P(-1) else Construction()\n",
    "        a2match_head = int(a2match.getrole('head', 0))\n",
    "        \n",
    "        common = {\n",
    "            \n",
    "            'w.name not in {qquant,card}':\n",
    "                word(w).name not in {'qquant','card'},\n",
    "            \n",
    "            'P(-1).name == cont':\n",
    "                word(P(-1)).name == 'cont',\n",
    "                        \n",
    "            'P(-1, st) & {NA, a}': \n",
    "                P(-1,'st') in {'NA', 'a'},   \n",
    "            \n",
    "            'P(-1).name != quant':\n",
    "                word(P(-1)).name != 'quant',\n",
    "            \n",
    "            'P(-1).name != prep':\n",
    "                word(P(-1)).name != 'prep',\n",
    "        }\n",
    "                \n",
    "        tests = (\n",
    "            \n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'adjv (1x)',\n",
    "                'roles': {'adjv':word(w), 'head': word(P(-1))},\n",
    "                'conds': dict(common, **{\n",
    "                    'F.sp.v(w) in {adjv, verb}':\n",
    "                        F.sp.v(w) in {'adjv', 'verb'},\n",
    "                })\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'adjv (2x)',\n",
    "                'roles': {'adjv': word(w), 'head': word(a2match_head)},\n",
    "                'conds': dict(common, **{\n",
    "                    \n",
    "                    'F.sp.v(w) in {adjv, verb}':\n",
    "                        F.sp.v(w) in {'adjv', 'verb'},\n",
    "                    \n",
    "                     'self.adjv(P(-1)) and target != P(0)':\n",
    "                        bool(a2match) and a2match_head != P(0)\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "\n",
    "        return self.test(*tests)\n",
    "     \n",
    "    def attrib(self, w):\n",
    "        \"\"\"Identify elements in a attrib construction.\n",
    "        \n",
    "        In Hebrew this construction typically consists of four slots:\n",
    "            > ה + A + ה + B\n",
    "        Attrib identifies each of these elements and labels them.\n",
    "        A is assumed to be the head, or modified, element and B\n",
    "        is assumed to be an adjectival element.\n",
    "        \"\"\"\n",
    "                \n",
    "        # CX consists of two constituent cxs\n",
    "        # start walk from head of first match\n",
    "        defi1 = self.defi(w)\n",
    "        d1head = int(defi1.getrole('head', 0))    \n",
    "        Wk = self.getWk(d1head)\n",
    "\n",
    "        # walk to next valid defi match\n",
    "        # and allow adjectives to intervene:\n",
    "        defi2 = Wk.ahead(\n",
    "            lambda n: self.defi(n),\n",
    "            go=lambda n: self.F.sp.v(n)=='adjv',\n",
    "            output=True\n",
    "        ) if Wk else Construction()\n",
    "        defi2 = defi2 or Construction()\n",
    "                            \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'attrib_ph',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'head': defi1, 'attrib': defi2},\n",
    "                'conds': {\n",
    "                    'bool(defi1)':\n",
    "                        bool(defi1),\n",
    "                    'bool(defi2)':\n",
    "                        bool(defi2), \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def numb(self, w):\n",
    "        \"\"\"Defines numerical relations with an non-quant word.\n",
    "        \n",
    "        Often but not always indicates quantification as other\n",
    "        semantic relations are possible.\n",
    "        \"\"\"\n",
    "\n",
    "        P = self.getP(w)\n",
    "        Wk = self.getWk(w)\n",
    "        word = self.word\n",
    "        is_nom = (\n",
    "            lambda n: word(n).name == 'cont'\n",
    "        )\n",
    "        is_prep = (\n",
    "            lambda n: word(n).name == 'prep'\n",
    "        )\n",
    "        behind_nom = Wk.back(is_nom, stop=lambda n: not is_nom(n)) \n",
    "        \n",
    "        return self.test(\n",
    "        \n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'numb_ph',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'numbered forward',\n",
    "                'roles': {'numb': word(w), 'head': word(P(1))},\n",
    "                'conds': {\n",
    "                    \n",
    "                    'w.name in {qquant,card}':\n",
    "                     word(w).name in {'qquant', 'card'},\n",
    "                    \n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1)),\n",
    "                    \n",
    "                    'P(1,sp) != conj':\n",
    "                        P(1,'sp') != 'conj',\n",
    "                    \n",
    "                    'P(1).name not in {qquant,card,prep}':\n",
    "                        word(P(1)).name not in {'qquant','card','prep'},\n",
    "        \n",
    "                    'P(-1,sp) != art':\n",
    "                        P(-1,'sp') != 'art',\n",
    "                },\n",
    "            },  \n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'numb_ph',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'numbered backward',\n",
    "                'roles': {'numb': word(w), 'head': word(behind_nom)},\n",
    "                'conds': {\n",
    "                    \n",
    "                    'w.name in {qquant,card}':\n",
    "                        word(w).name in {'qquant','card'},\n",
    "                    \n",
    "                    'not Wk.ahead(is_nominal)':\n",
    "                        not Wk.ahead(is_nom, stop=is_prep),\n",
    "                    \n",
    "                    'bool(Wk.back(is_nominal))':\n",
    "                        bool(behind_nom),\n",
    "                    \n",
    "                    'F.st.v(behind_nom) in {a, NA}':\n",
    "                        self.F.st.v(behind_nom) in {'a', 'NA'},\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "        \n",
    "    def card_chain(self, w):\n",
    "        \"\"\"Defines cardinal number chain constructions\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        F = self.F\n",
    "        word = self.word\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'card_chain',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'adjacent',\n",
    "                'roles': {'card':word(w), 'head':word(P(-1))},\n",
    "                'conds': {\n",
    "                    \n",
    "                    'F.ls.v(w) == card':\n",
    "                        F.ls.v(w) == 'card',\n",
    "                    'P(-1,ls) == card':\n",
    "                        P(-1,'ls') == 'card',                    \n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': 'card_chain',\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'conjunctive',\n",
    "                'roles': {'card': word(w), 'head': word(P(-2)), 'conj': word(P(-1))},\n",
    "                'conds': {\n",
    "                    'F.ls.v(w) == card':\n",
    "                        F.ls.v(w) == 'card',\n",
    "                    'P(-1,lex) == W':\n",
    "                        P(-1,'lex') == 'W',\n",
    "                    'P(-2,ls) == card':\n",
    "                        P(-2,'ls') == 'card',   \n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def demon(self, w):\n",
    "        \"\"\"Defines an adjacent demonstrative construction.\"\"\"\n",
    "        \n",
    "        P = self.getP(w)\n",
    "        word = self.word\n",
    "        F = self.F\n",
    "        name = 'demon_ph'\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'adjacent forward',\n",
    "                'roles': {'demon': word(w), 'head': word(P(1))},\n",
    "                'conds': {\n",
    "                    'prde in {F.pdp.v(w), F.sp.v(w)}':\n",
    "                        'prde' in {F.pdp.v(w), F.sp.v(w)},\n",
    "                    \n",
    "                    'P(-1,sp) != art': # ensure not part of attrib pattern\n",
    "                        P(-1,'sp') != 'art',\n",
    "                    \n",
    "                    'P(-1).name != prep':\n",
    "                        word(P(-1)).name != 'prep',\n",
    "                    \n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1)),\n",
    "                    \n",
    "                    'P(1).name == cont':\n",
    "                        word(P(1)).name == 'cont',\n",
    "                }\n",
    "            },\n",
    "            {\n",
    "                'element': w,\n",
    "                'name': name,\n",
    "                'kind': self.kind,\n",
    "                'pattern': 'adjacent back',\n",
    "                'roles': {'demon':word(w), 'head':word(P(-1))},\n",
    "                'conds': {\n",
    "                    'prde in {F.pdp.v(w), F.sp.v(w)}':\n",
    "                        'prde' in {F.pdp.v(w), F.sp.v(w)},\n",
    "                    \n",
    "                    'P(-1).name not in {prep,qquant,card}':\n",
    "                        word(P(-1)).name not in {'prep','qquant','card'},\n",
    "                    \n",
    "                    'P(-1,sp) == subs':\n",
    "                        P(-1,'sp') == 'subs',\n",
    "                }\n",
    "            }\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Constructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning word construction analysis...\n",
      "\t0:00:06.857731 COMPLETE \t[ 12887 ] words loaded\n"
     ]
    }
   ],
   "source": [
    "words = wordConstructions(A) # word CX builder\n",
    "\n",
    "# analyze all matches; return as dict\n",
    "start = datetime.now()\n",
    "print(f'Beginning word construction analysis...')\n",
    "wordcxs = words.cxdict(\n",
    "    s for tp in timephrases\n",
    "        for s in L.d(tp,'word')\n",
    ")\n",
    "print(f'\\t{datetime.now() - start} COMPLETE \\t[ {len(wordcxs)} ] words loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# time phrase CX builder\n",
    "tpc = TPConstructions(wordcxs, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TO FIX:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#pretty(1447386)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NB: L> is marked as the object of the preposition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Small Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_small = tpc.attrib(295413)\n",
    "#showcx(test_small, conds=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stretch Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = tpc.analyzestretch(L.d(1447669, 'word'), debug=True)\n",
    "\n",
    "# for res in test:\n",
    "#     showcx(res, conds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words = [w for ph in timephrases for w in L.d(ph, 'word')]\n",
    "\n",
    "# test_search(words, tpc.prep, pattern='prep...on', show=100, end=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on Random Phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuff = [k for k in timephrases\n",
    "            if len(L.d(k,'word')) > 4]\n",
    "random.shuffle(shuff)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for phrase in shuff[:25]:\n",
    "    \n",
    "#     print('analyzing', phrase)\n",
    "#     elements = L.d(phrase,'word')\n",
    "    \n",
    "#     try:\n",
    "#         cxs = tpc.analyzestretch(elements)\n",
    "#         if cxs:\n",
    "#             for cx in cxs:\n",
    "#                 showcx(cx, refslots=elements)\n",
    "#         else:\n",
    "#             showcx(Construction(), refslots=elements)\n",
    "    \n",
    "#     except:\n",
    "#         sys.stderr.write(f'\\nFAIL...running with debug...\\n')\n",
    "#         pretty(phrase)\n",
    "#         tpc.analyzestretch(elements, debug=True)\n",
    "#         raise Exception('...debug complete...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing on All Timephrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:00:00.000040 beginning analysis...\n",
      "\t0:00:09.729203\tdone with iter 500/3864\n",
      "\t0:00:20.002883\tdone with iter 1000/3864\n",
      "\t0:00:29.211148\tdone with iter 1500/3864\n",
      "\t0:00:38.087269\tdone with iter 2000/3864\n",
      "\t0:00:49.218074\tdone with iter 2500/3864\n",
      "\t0:01:00.729027\tdone with iter 3000/3864\n",
      "\t0:01:09.570685\tdone with iter 3500/3864\n",
      "0:01:18.804731\tCOMPLETE\n",
      "--------------------\n",
      "3864 phrases matched with Constructions...\n",
      "0 phrases not yet matched with Constructions...\n"
     ]
    }
   ],
   "source": [
    "phrase2cxs = collections.defaultdict(list)\n",
    "nocxs = []\n",
    "\n",
    "# time it\n",
    "start = datetime.now()\n",
    "\n",
    "print(f'{datetime.now()-start} beginning analysis...')\n",
    "\n",
    "for i, phrase in enumerate(timephrases):\n",
    "     \n",
    "    # analyze all known relas\n",
    "    elements = L.d(phrase,'word')\n",
    "    \n",
    "    # analyze with debug exceptions\n",
    "    try:\n",
    "        cxs = tpc.analyzestretch(elements)\n",
    "    except:\n",
    "        sys.stderr.write(f'\\nFAIL...running with debug...\\n')\n",
    "        pretty(phrase)\n",
    "        tpc.analyzestretch(elements, debug=True)\n",
    "        raise Exception('...debug complete...')\n",
    "\n",
    "    # save those phrases that have no matching constructions\n",
    "    if not cxs:\n",
    "        nocxs.append(phrase)\n",
    "    else:\n",
    "        phrase2cxs[phrase] = cxs\n",
    "        \n",
    "    # report status\n",
    "    if i % 500 == 0 and i:\n",
    "        print(f'\\t{datetime.now()-start}\\tdone with iter {i}/{len(timephrases)}')\n",
    "        \n",
    "print(f'{datetime.now()-start}\\tCOMPLETE')\n",
    "print('-'*20)\n",
    "print(f'{len(phrase2cxs)} phrases matched with Constructions...')\n",
    "print(f'{len(nocxs)} phrases not yet matched with Constructions...')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Closing Gaps\n",
    "\n",
    "### Identify Gaps\n",
    "\n",
    "Find timephrases that contain un-covered words besides waw conjunctions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 gapped phrases logged...\n"
     ]
    }
   ],
   "source": [
    "gapped = []\n",
    "tested = []\n",
    "\n",
    "for ph, cxs in phrase2cxs.items():\n",
    "    \n",
    "    tested.append(ph)\n",
    "    \n",
    "    ph_slots = set(\n",
    "        s for s in L.d(ph,'word')\n",
    "    )\n",
    "    cx_slots = set(\n",
    "        s for cx in cxs\n",
    "            for s in cx.slots\n",
    "    )\n",
    "    \n",
    "    if ph_slots.difference(cx_slots):\n",
    "        gapped.append(cxs)\n",
    "        \n",
    "print(f'{len(gapped)} gapped phrases logged...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gp in gapped[:25]:\n",
    "    for cx in gp:\n",
    "        showcx(cx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connecting Constructions\n",
    "\n",
    "Developing a CXbuilder to connect all constructions in a complete phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CXbuilderPH(CXbuilder):\n",
    "    \"\"\"Build complete phrase constructions.\"\"\"\n",
    "    \n",
    "    def __init__(self, phrase2cxs, semdists, tf):\n",
    "        CXbuilder.__init__(self)\n",
    "        \n",
    "        # set up tf methods\n",
    "        self.tf = tf\n",
    "        self.F, self.T, self.L = tf.api.F, tf.api.T, tf.api.L\n",
    "        \n",
    "        # map cx to phrase node for context retrieval\n",
    "        self.cx2phrase = {\n",
    "            cx:ph \n",
    "                for ph in phrase2cxs\n",
    "                    for cx in phrase2cxs[ph]\n",
    "        }\n",
    "        \n",
    "        self.phrase2cxs = phrase2cxs\n",
    "        self.semdists = semdists\n",
    "        \n",
    "        self.cxs = (        \n",
    "            self.plusprep,\n",
    "            self.adjacent\n",
    "        )\n",
    "        self.dripbucket = (\n",
    "            self.cxph,\n",
    "        )\n",
    "        \n",
    "        self.kind = 'phrase'\n",
    "        \n",
    "    def cxph(self, cx):\n",
    "        \"\"\"Dripbucket function that returns cx as is.\"\"\"\n",
    "        return cx\n",
    "        \n",
    "    def get_context(self, cx):\n",
    "        \"\"\"Get context for a given cx.\"\"\"\n",
    "        phrase = self.cx2phrase.get(cx, None)\n",
    "        if phrase:\n",
    "            return self.phrase2cxs[phrase]\n",
    "        else:\n",
    "            return tuple()\n",
    "        \n",
    "    def getP(self, cx):\n",
    "        \"\"\"Index positions on phrase context\"\"\"\n",
    "        positions = self.get_context(cx)\n",
    "        if positions:\n",
    "            return Positions(\n",
    "                cx, positions, default=Construction()\n",
    "            ).get\n",
    "        else:\n",
    "            return Dummy\n",
    "\n",
    "    def getWk(self, cx):\n",
    "        \"\"\"Index walks on phrase context\"\"\"\n",
    "        positions = self.get_context(cx)\n",
    "        if positions:\n",
    "            return Walker(cx, positions)\n",
    "        else:\n",
    "            return Dummy()\n",
    "    \n",
    "    def getindex(\n",
    "        self, indexable, index, \n",
    "        default=Construction()\n",
    "    ):\n",
    "        \"\"\"Safe index on iterables w/out IndexErrors.\"\"\"\n",
    "        try:\n",
    "            return indexable[index]\n",
    "        except:\n",
    "            return default\n",
    "    \n",
    "    def getname(self, cx):\n",
    "        \"\"\"Get a cx name\"\"\"\n",
    "        return cx.name\n",
    "    \n",
    "    def getkind(self, cx):\n",
    "        \"\"\"Get a cx kind.\"\"\"\n",
    "        return cx.kind\n",
    "    \n",
    "    def getsuccrole(self, cx, role, index=-1):\n",
    "        \"\"\"Get a cx role from a list of successive roles.\n",
    "        \n",
    "        e.g.\n",
    "        [big_head, medium_head, small_head][-1] == small_head\n",
    "        \"\"\"\n",
    "        cands = list(cx.getsuccroles(role))\n",
    "        try:\n",
    "            return cands[index]\n",
    "        except IndexError:\n",
    "            return Construction()\n",
    "    \n",
    "    def string_plus(self, cx, plus=1):\n",
    "        \"\"\"Stringifies a CX + N-slots for Levenshtein tests.\"\"\"\n",
    "        \n",
    "        # get all slots in the context for plussing\n",
    "        allslots = sorted(set(\n",
    "            s for scx in self.get_context(cx)\n",
    "                for s in scx.slots\n",
    "        ))\n",
    "        \n",
    "        # get plus slots\n",
    "        P = (Positions(self.getindex(cx.slots, -1), allslots).get\n",
    "                 if cx.slots and allslots else Dummy)\n",
    "        plusses = []\n",
    "        for i in range(plus, plus+1):\n",
    "            plusses.append(P(i,-1)) # -1 for null slots (== empty string in T.text)\n",
    "        plusses = [p for p in plusses if type(p) == int]\n",
    "        \n",
    "        # format the text string for Levenshtein testing\n",
    "        ptxt = T.text(\n",
    "            cx.slots + tuple(plusses),\n",
    "            fmt='text-orig-plain'\n",
    "        ) if cx.slots else ''\n",
    "        \n",
    "        return ptxt\n",
    "    \n",
    "    def plusprep(self, cx):\n",
    "        \"\"\"Find phrase+prep CXs\"\"\"\n",
    "        \n",
    "        P = self.getP(cx)\n",
    "                \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': cx,\n",
    "                'name': '+prep',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'+prep': cx, 'head': P(-1)},\n",
    "                'conds': {\n",
    "                    'cx.name == prep_ph':\n",
    "                        cx.name == 'prep_ph',\n",
    "                    'bool(P(-1))':\n",
    "                        bool(P(-1)),\n",
    "                    'P(-1,name) != conj':\n",
    "                        P(-1).name != 'conj',\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def adjacent(self, cx):\n",
    "        \"\"\"Find adjacent CXs\"\"\"\n",
    "        \n",
    "        P = self.getP(cx)\n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': cx,\n",
    "                'name': 'adjacent',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'phrase1':cx, 'phrase2':P(1)},\n",
    "                'conds': {\n",
    "                    'cx.name not in {conj,prep}':\n",
    "                        cx.name not in {'conj','prep_ph'},\n",
    "                    'bool(P(1))':\n",
    "                        bool(P(1)),\n",
    "                    'P(1,name) not in {conj, prep_ph}':\n",
    "                        P(1,self.getname) not in {'conj','prep_ph'},\n",
    "                    'not appo_name(P(1))':\n",
    "                        not (self.appo_name(P(1)) if P(1) else False),\n",
    "                    'not appo_name(cx)':\n",
    "                        not self.appo_name(cx),\n",
    "                }\n",
    "            }\n",
    "        \n",
    "        )\n",
    "    \n",
    "    def coord(self, cx):\n",
    "        \"\"\"A coordinate construction.\n",
    "        \n",
    "        In order to match a coordinate cx, we need to determine\n",
    "        which item in the previous phrase this cx belongs with. \n",
    "        This is done using a semantic vector space, which can\n",
    "        quantify the approximate semantic distance between the\n",
    "        heads of this cx and a candidate cx.\n",
    "        \n",
    "        Criteria utilized in validating a coordinate cx between\n",
    "        an origin cx and a candidate cx are the following:\n",
    "            TODO: fill in\n",
    "        \"\"\"\n",
    "        \n",
    "        F, T = self.F, self.T\n",
    "        P = self.getP(cx)\n",
    "        semdist = self.semdists\n",
    "        Wk = self.getWk(cx)\n",
    "                         \n",
    "        # get all top-level cxs behind this one that match in name\n",
    "        cx_behinds = Wk.back(\n",
    "            lambda c: c.name == cx.name,\n",
    "            every=True,\n",
    "            stop=lambda c: (\n",
    "                c.name == 'conj' and (c != P(-1))\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        # if top level phrases produce no results,\n",
    "        # use subphrases instead\n",
    "        if not cx_behinds:\n",
    "            topcontext = self.get_context(cx)\n",
    "            \n",
    "            # gather all valid subphrase candidates\n",
    "            subcontext = []\n",
    "            for topcx in topcontext:\n",
    "                for subcx in topcx.subgraph():\n",
    "                    if type(subcx) == int: # skip TF slots\n",
    "                        continue\n",
    "                    if (\n",
    "                        subcx in topcontext or subcx.name != 'conj'\n",
    "                        and subcx not in cx\n",
    "                    ):\n",
    "                        subcontext.append(subcx)        \n",
    "            \n",
    "            # walk the new candidates\n",
    "            Wk2 = Walker(cx, subcontext)\n",
    "            cx_behinds = Wk2.back(\n",
    "                lambda c: c.name != 'conj', \n",
    "                default=[P(-2)],\n",
    "                every=True,\n",
    "                stop=lambda c: (\n",
    "                    c.name == 'conj' and (c != P(-1))\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        # map each back-cx to its last slot to make sure\n",
    "        # every candidate is the last item in its phrase\n",
    "        # check is made in next series of lines\n",
    "        cx2last = {\n",
    "            cxb:self.getindex(sorted(cxb.slots), -1, 0)\n",
    "                for cxb in cx_behinds\n",
    "        }\n",
    "        \n",
    "        # find coordinate candidate subphrases that stand\n",
    "        # at the end of the phrase\n",
    "        cx_subphrases = []\n",
    "        \n",
    "        for cx_back in cx_behinds:\n",
    "            for cxsp in cx_back.subgraph():\n",
    "                if type(cxsp) == int:\n",
    "                    continue\n",
    "                elif (\n",
    "                    cx2last[cx_back] in cxsp.slots\n",
    "                    and cxsp.getrole('head')\n",
    "                ):\n",
    "                    cx_subphrases.append(cxsp)\n",
    "        \n",
    "        # get subphrase heads for semantic tests\n",
    "        cx2heads = [\n",
    "            (cxsp, self.getsuccrole(cxsp,'head'))\n",
    "                for cxsp in cx_behinds\n",
    "        ]\n",
    "\n",
    "        # get head of this cx\n",
    "        head1 = self.getsuccrole(cx,'head')     \n",
    "        head1lex = F.lex.v(head1)\n",
    "        \n",
    "        # sort on a set of priorities\n",
    "        # the default sort behavior is used (least to greatest)\n",
    "        # thus when a bigger value should be more important, \n",
    "        # a negative is added to the number\n",
    "        stringp = self.string_plus\n",
    "        \n",
    "        # arrange candidates by priority\n",
    "        cxpriority = []\n",
    "        for cxsp, headsp in cx2heads:\n",
    "            name_eq = 0 if cxsp.name == cx.name else 1\n",
    "            semantic_dist = semdist.get(\n",
    "                head1lex,{}\n",
    "            ).get(F.lex.v(headsp), np.inf)\n",
    "            size = -len(cxsp.slots)\n",
    "            levenshtein = lev_dist(stringp(cx), stringp(cxsp))\n",
    "            slot_dist = -next(iter(cxsp.slots), 0)\n",
    "            heads = (head1, headsp) # for reporting purposes only\n",
    "            \n",
    "            cxpriority.append((\n",
    "                name_eq,\n",
    "                semantic_dist,\n",
    "                size,\n",
    "                levenshtein,\n",
    "                slot_dist,\n",
    "                heads,\n",
    "                cxsp\n",
    "            ))\n",
    "            \n",
    "        # make the sorting\n",
    "        cxpriority = sorted(cxpriority, key=lambda k: k[:-1])\n",
    "        \n",
    "        # select the first priority candidate\n",
    "        cand = next(iter(cxpriority), (0,0,Construction()))\n",
    "        \n",
    "        # add data for conds report / debugging\n",
    "        data = collections.defaultdict(str)\n",
    "        for namescore,sdist,leng,ldist,lslot,heads,cxp in cxpriority:\n",
    "            # name equality\n",
    "            data['namescore'] += f'\\n\\t{cxp} namescore: {namescore}'\n",
    "            # semantic distance\n",
    "            data['semdists'] += (\n",
    "                f'\\n\\t{round(sdist, 2)}, {F.lex.v(heads[0])} ~ {F.lex.v(heads[1])}, {cxp}'\n",
    "            )\n",
    "            # size of cx\n",
    "            data['size'] += f'\\n\\t{cxp} length: {abs(leng)}'\n",
    "            \n",
    "            # Levenstein distance\n",
    "            data['ldist'] += f'\\n\\t{cxp} dist: {ldist}'\n",
    "            \n",
    "            # dist of last slot\n",
    "            data['lslot'] += f'\\n\\t{cxp} last slot: {abs(lslot)}'\n",
    "    \n",
    "        \n",
    "        return self.test(\n",
    "            {\n",
    "                'element': cx,\n",
    "                'name': 'coord',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'part2':cx, 'conj': P(-1), 'part1': cand[-1]},\n",
    "                'conds': {\n",
    "                    'P(-1).name == conj':\n",
    "                        P(-1).name == 'conj',\n",
    "                    'bool(cand)':\n",
    "                        bool(cand[-1]),\n",
    "                    f'name matches {data[\"namescore\"]}\\n':\n",
    "                        bool(cxpriority),\n",
    "                    f'is shortest sem. distance of {data[\"semdists\"]}\\n':\n",
    "                        bool(cxpriority),\n",
    "                    f'is longest length of: {data[\"size\"]}\\n':\n",
    "                        bool(cxpriority),\n",
    "                    f'is shortest Levenshtein distance: {data[\"ldist\"]}\\n':\n",
    "                        bool(cxpriority),\n",
    "                    f'is closest last slot of: {data[\"lslot\"]}\\n':\n",
    "                        bool(cxpriority)\n",
    "                }\n",
    "            }\n",
    "            \n",
    "            # NB Need a separate pattern for word-level connections\n",
    "            # e.g. יום ולילה are missed by above pattern due to head1 rela\n",
    "        )\n",
    "    \n",
    "    def appo_name(self, cx):\n",
    "        \"\"\"Apposition of name\"\"\"\n",
    "        \n",
    "        P = self.getP(cx)\n",
    "        \n",
    "        try:\n",
    "            cxhead = self.getrole(cx, 'head', index=-2)\n",
    "        except: \n",
    "            raise Exception(cx)\n",
    "        \n",
    "        # get very last embedded cx in P(-1)\n",
    "        namecx = self.getindex(\n",
    "            self.sortbyslot(P(-1).unfoldcxs()),\n",
    "            -1\n",
    "        )\n",
    "        \n",
    "        # get slots for tests\n",
    "        first_slot = sorted(cx.slots)[0] # of this cx\n",
    "        head_slot = int(cxhead) if cxhead else 0\n",
    "        name_slot = int(namecx) if namecx else 0\n",
    "        \n",
    "        return self.test(\n",
    "        \n",
    "            {\n",
    "                'element': cx,\n",
    "                'name': 'appo_name',\n",
    "                'kind': self.kind,\n",
    "                'roles': {'name': cx, 'head':namecx},\n",
    "                'conds': {\n",
    "                    \n",
    "                    'cx(head).name == cont':\n",
    "                        cxhead.name == 'cont',\n",
    "                    \n",
    "                    'cx.name not in {prep_ph}':\n",
    "                        cx.name not in {'prep_ph'},\n",
    "                    \n",
    "                    'bool(P(-1))':\n",
    "                        bool(P(-1)),\n",
    "                    \n",
    "                    'backcx.name == name':\n",
    "                        namecx.name == 'name',\n",
    "                    \n",
    "                    f'F.nu.v({head_slot}) == F.nu.v({name_slot})':\n",
    "                        F.nu.v(head_slot) == F.nu.v(name_slot),\n",
    "                    \n",
    "                    'head_slot == first_slot or first_slot==art':\n",
    "                        (\n",
    "                            head_slot == first_slot\n",
    "                            or self.F.sp.v(first_slot) == 'art'\n",
    "                        ),\n",
    "                    \n",
    "                    # NB:\n",
    "                    # rule below reveals the need to be able to say\n",
    "                    # what head_slot should be; i.e., the lexeme should\n",
    "                    # be semantically consistent with the ID of the proper name\n",
    "                    # if person, head_slot should ~ person, etc.\n",
    "                    'F.lex.v(head_slot) not in timeword set':\n",
    "                        F.lex.v(head_slot) not in {'CNH/'}\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "cxp = CXbuilderPH(phrase2cxs, semdist, A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following phrases contain cases that still\n",
    "# need to be fixed for the coordinate cx; some should\n",
    "# actually be done in the previous cx builder at subphrase level\n",
    "coord_tofix = [\n",
    "    1450039, # add adjacent advb cx with JWM\n",
    "    1450075, # add adjacent advb cx with >Z\n",
    "    1450647, # consider prioritizing Levenshtein over size\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[CX prep_ph (423080, 423081, 423082),\n",
       " CX conj (423083,),\n",
       " CX attrib_ph (423084, 423085, 423086, 423087)]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testph = phrase2cxs[coord_tofix[-1]]\n",
    "testph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cxp.coord(testph[-1])\n",
    "\n",
    "# showcx(test, conds=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pattern Matches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filt_gaps(cx):\n",
    "    \"\"\"Isolate cxs with gaps\"\"\"\n",
    "    timephrase = L.u(next(iter(cx.slots)),'phrase')[0]\n",
    "    if set(L.d(timephrase,'word')) - cx.slots:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "    \n",
    "def filt(cx):\n",
    "    \"\"\"Find specific lexeme\"\"\"\n",
    "    timephrase = L.u(next(iter(cx.slots)),'phrase')[0]\n",
    "    if '>XR/' in set(F.lex.v(w) for w in L.d(timephrase,'word')):\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# elements = [\n",
    "#     cx for ph in list(phrase2cxs.values())\n",
    "#         for cx in ph\n",
    "# ]\n",
    "\n",
    "# test_search(\n",
    "#     elements, \n",
    "#     cxp.coord, \n",
    "#     pattern='', \n",
    "#     shuffle=False,\n",
    "# #     select=lambda c: not filt_gaps(c),\n",
    "#     extraFeatures='lex st',\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stretch Tests\n",
    "\n",
    "Testing across a whole phrase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test = cxp.analyzestretch(phrase2cxs[1449168], debug=True)\n",
    "# for res in test:\n",
    "#     showcx(res, conds=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
